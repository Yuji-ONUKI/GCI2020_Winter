{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "July02_0300_0.7445.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuji-ONUKI/GCI2020_Winter/blob/main/July02_0300_0_7445.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print('Importing data...')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/GCI/02.（公開）コンペ2-20220621T094535Z-001.zip (Unzipped Files)/02.（公開）コンペ2/input/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/GCI/02.（公開）コンペ2-20220621T094535Z-001.zip (Unzipped Files)/02.（公開）コンペ2/input/test.csv\")"
      ],
      "metadata": {
        "_uuid": "1ff174931af1e4c26e808ab542c8d0a299d08c09",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HehSKiyFonMe",
        "outputId": "18c9b263-ed50-4a2c-e6b7-b16005a00291"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an anomalous flag column\n",
        "data['DAYS_EMPLOYED_ANOM'] = data[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "data['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "# Create an anomalous flag column\n",
        "test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)"
      ],
      "metadata": {
        "_uuid": "1fe25018b69513696fa11a9f75d548e95010ffa2",
        "trusted": true,
        "id": "y40rbuRAonMq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "COyKL4UZZZYc"
      },
      "outputs": [],
      "source": [
        "data['RT_CREDIT']=data['AMT_CREDIT']/data['AMT_INCOME_TOTAL']\n",
        "test['RT_CREDIT']=test['AMT_CREDIT']/test['AMT_INCOME_TOTAL']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=100\n",
        "data['RT_CREDIT']=round(data['RT_CREDIT']*n)/n\n",
        "len(data['RT_CREDIT'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6vgPpnQnNq5",
        "outputId": "4ed0e452-dfce-4c7b-9f39-3e8c8f3b9636"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1787"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols =list(test.columns)\n",
        "cols"
      ],
      "metadata": {
        "_uuid": "d39bac7ac55b2843f952111194508968bad6597f",
        "trusted": true,
        "id": "TWS2OXgqonMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca363b1-84a5-4679-8ea9-ea566e458b31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SK_ID_CURR',\n",
              " 'NAME_CONTRACT_TYPE',\n",
              " 'CODE_GENDER',\n",
              " 'FLAG_OWN_CAR',\n",
              " 'FLAG_OWN_REALTY',\n",
              " 'CNT_CHILDREN',\n",
              " 'AMT_INCOME_TOTAL',\n",
              " 'AMT_CREDIT',\n",
              " 'AMT_ANNUITY',\n",
              " 'AMT_GOODS_PRICE',\n",
              " 'NAME_TYPE_SUITE',\n",
              " 'NAME_INCOME_TYPE',\n",
              " 'NAME_EDUCATION_TYPE',\n",
              " 'NAME_FAMILY_STATUS',\n",
              " 'NAME_HOUSING_TYPE',\n",
              " 'REGION_POPULATION_RELATIVE',\n",
              " 'DAYS_BIRTH',\n",
              " 'DAYS_EMPLOYED',\n",
              " 'DAYS_REGISTRATION',\n",
              " 'DAYS_ID_PUBLISH',\n",
              " 'OWN_CAR_AGE',\n",
              " 'FLAG_MOBIL',\n",
              " 'FLAG_EMP_PHONE',\n",
              " 'FLAG_WORK_PHONE',\n",
              " 'FLAG_CONT_MOBILE',\n",
              " 'FLAG_PHONE',\n",
              " 'FLAG_EMAIL',\n",
              " 'OCCUPATION_TYPE',\n",
              " 'CNT_FAM_MEMBERS',\n",
              " 'REGION_RATING_CLIENT',\n",
              " 'REGION_RATING_CLIENT_W_CITY',\n",
              " 'REG_REGION_NOT_LIVE_REGION',\n",
              " 'REG_REGION_NOT_WORK_REGION',\n",
              " 'LIVE_REGION_NOT_WORK_REGION',\n",
              " 'REG_CITY_NOT_LIVE_CITY',\n",
              " 'REG_CITY_NOT_WORK_CITY',\n",
              " 'LIVE_CITY_NOT_WORK_CITY',\n",
              " 'ORGANIZATION_TYPE',\n",
              " 'EXT_SOURCE_1',\n",
              " 'EXT_SOURCE_2',\n",
              " 'EXT_SOURCE_3',\n",
              " 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
              " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
              " 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
              " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
              " 'DAYS_LAST_PHONE_CHANGE',\n",
              " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
              " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
              " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
              " 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
              " 'DAYS_EMPLOYED_ANOM',\n",
              " 'RT_CREDIT']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YbB7BC-ap5iK"
      },
      "outputs": [],
      "source": [
        "data['YEARS_BIRTH']=round(data['DAYS_BIRTH']/365)\n",
        "test['YEARS_BIRTH']=round(test['DAYS_BIRTH']/365)\n",
        "\n",
        "data['YEARS_ID_PUBLISH']=round(data['DAYS_ID_PUBLISH']/365)\n",
        "test['YEARS_ID_PUBLISH']=round(test['DAYS_ID_PUBLISH']/365)\n",
        "\n",
        "data['YEARS_REGISTRATION']=round(data['DAYS_REGISTRATION']/365)\n",
        "test['YEARS_REGISTRATION']=round(test['DAYS_REGISTRATION']/365)\n",
        "\n",
        "data['YEARS_EMPLOYED']=round(data['DAYS_EMPLOYED']/365)\n",
        "test['YEARS_EMPLOYED']=round(test['DAYS_EMPLOYED']/365)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3sqbzKd94syY"
      },
      "outputs": [],
      "source": [
        "data['AGE_EMP']=data['YEARS_BIRTH']-data['YEARS_EMPLOYED']\n",
        "test['AGE_EMP']=test['YEARS_BIRTH']-test['YEARS_EMPLOYED']\n",
        "\n",
        "# 26歳までの上昇局面とそれ以降の下降局面を別のカラムにする\n",
        "data['AGE_EMP1']=np.NAN\n",
        "test['AGE_EMP1']=np.NAN\n",
        "data.loc[data['AGE_EMP']<-26,'AGE_EMP1']=data['AGE_EMP']\n",
        "test.loc[test['AGE_EMP']<-26,'AGE_EMP1']=test['AGE_EMP']\n",
        "data.loc[data['AGE_EMP']<-26,'AGE_EMP']=np.NAN\n",
        "test.loc[test['AGE_EMP']<-26,'AGE_EMP']=np.NAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvbVtWnV-ksW",
        "outputId": "f5e373bc-daeb-41f7-ebd9-a57e756fb1c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.072508, 0.04622 ])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data.loc[data['REGION_POPULATION_RELATIVE']>0.04,'REGION_POPULATION_RELATIVE'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0RnkvlFI-mxA"
      },
      "outputs": [],
      "source": [
        "data['REGION_POPULATION_RELATIVE_0.04622']=0\n",
        "data['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE_0.04622']=1\n",
        "data['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "test['REGION_POPULATION_RELATIVE_0.04622']=0\n",
        "test['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE_0.04622']=1\n",
        "test['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "data['REGION_POPULATION_RELATIVE_0.072508']=0\n",
        "data['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE_0.072508']=1\n",
        "data['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "test['REGION_POPULATION_RELATIVE_0.072508']=0\n",
        "test['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE_0.072508']=1\n",
        "test['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE']=np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o3KNWUU8yasX"
      },
      "outputs": [],
      "source": [
        "data['OWN_CAR_AGE_64']=0\n",
        "data['OWN_CAR_AGE'==64,'OWN_CAR_AGE_64']=1\n",
        "data['OWN_CAR_AGE'==64,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "test['OWN_CAR_AGE_64']=0\n",
        "test['OWN_CAR_AGE'==64,'OWN_CAR_AGE_64']=1\n",
        "test['OWN_CAR_AGE'==64,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "data['OWN_CAR_AGE_65']=0\n",
        "data['OWN_CAR_AGE'==65,'OWN_CAR_AGE_65']=1\n",
        "data['OWN_CAR_AGE'==65,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "test['OWN_CAR_AGE_65']=0\n",
        "test['OWN_CAR_AGE'==65,'OWN_CAR_AGE_65']=1\n",
        "test['OWN_CAR_AGE'==65,'OWN_CAR_AGE']=np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v29vVfC0ljQm"
      },
      "outputs": [],
      "source": [
        "data['LOW_DEFAULT_ORG']=0\n",
        "data.loc[data['ORGANIZATION_TYPE']=='Industry: type 12','LOW_DEFAULT_ORG' ]=1\n",
        "data.loc[data['ORGANIZATION_TYPE']=='Trade: type 4','LOW_DEFAULT_ORG' ]=1\n",
        "test['LOW_DEFAULT_ORG']=0\n",
        "test.loc[test['ORGANIZATION_TYPE']=='Industry: type 12','LOW_DEFAULT_ORG' ]=1\n",
        "test.loc[test['ORGANIZATION_TYPE']=='Trade: type 4','LOW_DEFAULT_ORG' ]=1\n",
        "\n",
        "data['HIGH_DEFAULT_ORG']=0\n",
        "data.loc[data['ORGANIZATION_TYPE']=='Transport: type 3','HIGH_DEFAULT_ORG']=1\n",
        "test['HIGH_DEFAULT_ORG']=0\n",
        "test.loc[test['ORGANIZATION_TYPE']=='Transport: type 3','HIGH_DEFAULT_ORG']=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['CREDIT_INCOME_PERCENT'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
        "test['ANNUITY_INCOME_PERCENT'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n"
      ],
      "metadata": {
        "id": "ZsFlpjr3pa9d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VMv3GVRdNWe_"
      },
      "outputs": [],
      "source": [
        "dict={}\n",
        "for item in data['AMT_INCOME_TOTAL'].unique():\n",
        "  dict[item]=data.loc[data['AMT_INCOME_TOTAL']==item,'SK_ID_CURR'].count()\n",
        "df = pd.DataFrame.from_dict(dict,orient='index')\n",
        "\n",
        "items = list(df[df[0]>20])\n",
        "\n",
        "data['AMT_INCOME_TOTAL_2']=np.NAN\n",
        "test['AMT_INCOME_TOTAL_2']=np.NAN\n",
        "for item in items:\n",
        "  data.loc[data['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL_2']=data.loc[data['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL']\n",
        "  test.loc[test['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL_2']=test.loc[test['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL']\n",
        "\n",
        "for item in items:\n",
        "  data.loc[data['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL']=np.NAN\n",
        "  test.loc[test['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL']=np.NAN\n",
        "\n",
        "data['AMT_INCOME_TOTAL']=round(data['AMT_INCOME_TOTAL']/1000)*1000\n",
        "test['AMT_INCOME_TOTAL']=round(test['AMT_INCOME_TOTAL']/1000)*1000\n",
        "\n",
        "data.loc[data['AMT_INCOME_TOTAL'].isna(),'AMT_INCOME_TOTAL']=data.loc[data['AMT_INCOME_TOTAL'].isna(),'AMT_INCOME_TOTAL_2']\n",
        "test.loc[test['AMT_INCOME_TOTAL'].isna(),'AMT_INCOME_TOTAL']=test.loc[test['AMT_INCOME_TOTAL'].isna(),'AMT_INCOME_TOTAL_2']\n",
        "\n",
        "data.drop(['AMT_INCOME_TOTAL_2'],axis=1,inplace=True)\n",
        "test.drop(['AMT_INCOME_TOTAL_2'],axis=1,inplace=True)\n",
        "\n",
        "data=pd.get_dummies(data,columns=['AMT_INCOME_TOTAL'])\n",
        "test=pd.get_dummies(test,columns={'AMT_INCOME_TOTAL'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5pYCjGXmwz-2"
      },
      "outputs": [],
      "source": [
        "# 欠測値であれば'_NAN'を加えたカラムを作る\n",
        "def flag_isNan(column_target):\n",
        "  if type(column_target)==str:\n",
        "    data[column_target+'_NAN']=0\n",
        "    test[column_target+'_NAN']=0\n",
        "    #\n",
        "    data.loc[data[column_target].isna(),column_target+'_NAN']=1\n",
        "    test.loc[test[column_target].isna(),column_target+'_NAN']=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UtcXCBAGzfQZ"
      },
      "outputs": [],
      "source": [
        "for col in [\n",
        " 'AMT_CREDIT',\n",
        " 'AMT_ANNUITY',\n",
        " 'AMT_GOODS_PRICE',\n",
        " 'REGION_POPULATION_RELATIVE',\n",
        " 'DAYS_REGISTRATION',\n",
        " 'OWN_CAR_AGE',\n",
        " 'CNT_FAM_MEMBERS',\n",
        "'EXT_SOURCE_1',\n",
        "'EXT_SOURCE_2',\n",
        "'EXT_SOURCE_3',\n",
        " 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
        " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
        " 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
        " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
        " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
        " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
        " 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
        " 'DAYS_BIRTH',\n",
        " 'DAYS_ID_PUBLISH',\n",
        " 'DAYS_EMPLOYED',\n",
        " (False, 'REGION_POPULATION_RELATIVE'),\n",
        " (False, 'OWN_CAR_AGE')\n",
        " ]:\n",
        "  if (test.loc[test[col].isna(),'SK_ID_CURR'].count()>0):\n",
        "    flag_isNan(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NOeyCa9ylhzq"
      },
      "outputs": [],
      "source": [
        "for col in list(test.dtypes[test.dtypes!='float'].keys()):\n",
        "  if (test.loc[test[col].isna(),'SK_ID_CURR'].count()>0):\n",
        "    flag_isNan(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zpaSJddDn5TT"
      },
      "outputs": [],
      "source": [
        "data['ROUND_RT_CREDIT']=round(data['RT_CREDIT'])\n",
        "data['ROUND_RT_CREDIT'].fillna(0,inplace=True)\n",
        "data.loc[data['ROUND_RT_CREDIT']>6,'ROUND_RT_CREDIT']=6\n",
        "data['ROUND_RT_CREDIT']=data['ROUND_RT_CREDIT'].astype(str)\n",
        "\n",
        "data=pd.get_dummies(data,columns=['ROUND_RT_CREDIT'])\n",
        "\n",
        "test['ROUND_RT_CREDIT']=round(test['RT_CREDIT'])\n",
        "test['ROUND_RT_CREDIT'].fillna(0,inplace=True)\n",
        "test.loc[test['ROUND_RT_CREDIT']>6,'ROUND_RT_CREDIT']=6\n",
        "test['ROUND_RT_CREDIT']=test['ROUND_RT_CREDIT'].astype(str)\n",
        "\n",
        "test=pd.get_dummies(test,columns=['ROUND_RT_CREDIT'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['ROUND_EXT_SOURCE_1']=round(data['EXT_SOURCE_1']*100)\n",
        "data['ROUND_EXT_SOURCE_2']=round(data['EXT_SOURCE_2']*100)\n",
        "data['ROUND_EXT_SOURCE_3']=round(data['EXT_SOURCE_3']*100)\n",
        "\n",
        "test['ROUND_EXT_SOURCE_1']=round(test['EXT_SOURCE_1']*100)\n",
        "test['ROUND_EXT_SOURCE_2']=round(test['EXT_SOURCE_2']*100)\n",
        "test['ROUND_EXT_SOURCE_3']=round(test['EXT_SOURCE_3']*100)"
      ],
      "metadata": {
        "id": "5cJpga-BIi85"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1=data.loc[data['TARGET']==1]\n",
        "t0=data.loc[data['TARGET']==0]\n",
        "ta=data\n",
        "def addColumnOfNumber(column_target):\n",
        "    column_new = 'NEW_NUM_'+column_target\n",
        "    cats = ta[column_target].unique()\n",
        "\n",
        "    dict={}\n",
        "    for cat in cats:\n",
        "      dict[cat]=(t1.loc[ta[column_target]==cat,'SK_ID_CURR'].count()/ta.loc[ta[column_target]==cat,'SK_ID_CURR'].count()).astype(str)\n",
        "\n",
        "    data[column_new]=data[column_target]\n",
        "    test[column_new]=test[column_target]\n",
        "\n",
        "    data[column_new]=data[column_new].map(dict)\n",
        "    test[column_new]=test[column_new].map(dict)\n",
        "\n",
        "    data[column_new]=data[column_new].astype(float)\n",
        "    test[column_new]=test[column_new].astype(float)"
      ],
      "metadata": {
        "id": "9EGH5oBkDAwL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#addColumnOfNumber('REALTY_INCOME_CREDIT')\n",
        "#addColumnOfNumber('ROUND_EXT_SOURCE_1')\n",
        "#addColumnOfNumber('ROUND_EXT_SOURCE_2')\n",
        "#addColumnOfNumber('ROUND_EXT_SOURCE_3')\n",
        "#data.drop(['ROUND_EXT_SOURCE_1','ROUND_EXT_SOURCE_2','ROUND_EXT_SOURCE_3'],axis=1,inplace=True)\n",
        "#test.drop(['ROUND_EXT_SOURCE_1','ROUND_EXT_SOURCE_2','ROUND_EXT_SOURCE_3'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "7wNq5TNgDPIo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
        "#test['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
        "\n",
        "data['YEARS_LAST_PHONE_CHANGE']=round(data['DAYS_LAST_PHONE_CHANGE']/365)\n",
        "test['YEARS_LAST_PHONE_CHANGE']=round(test['DAYS_LAST_PHONE_CHANGE']/365)\n",
        "\n",
        "data['MONTH_LAST_PHONE_CHANGE']=round(data['DAYS_LAST_PHONE_CHANGE']/30)\n",
        "test['MONTH_LAST_PHONE_CHANGE']=round(test['DAYS_LAST_PHONE_CHANGE']/30)"
      ],
      "metadata": {
        "_uuid": "f655026a52241ad6f63bd8260420edb648790bd9",
        "trusted": true,
        "id": "Ozd_2gLronMr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['CREDIT_TERM'] = data['AMT_CREDIT']/data['AMT_ANNUITY']\n",
        "test['CREDIT_TERM'] = test['AMT_CREDIT']/test['AMT_ANNUITY']\n",
        "data['CREDIT_REPAY'] = data['CREDIT_TERM'] *30 + data['DAYS_BIRTH']\n",
        "test['CREDIT_REPAY'] = test['CREDIT_TERM'] *30 + test['DAYS_BIRTH']\n",
        "\n",
        "data['NEW_CREDIT_TO_GOODS_RATIO'] = data['AMT_CREDIT'] / data['AMT_GOODS_PRICE']\n",
        "test['NEW_CREDIT_TO_GOODS_RATIO'] = test['AMT_CREDIT'] / test['AMT_GOODS_PRICE']\n",
        "data.loc[data['NAME_CONTRACT_TYPE']!='Cash loans','NEW_CREDIT_TO_GOODS_RATIO']=np.NAN\n",
        "test.loc[test['NAME_CONTRACT_TYPE']!='Cash loans','NEW_CREDIT_TO_GOODS_RATIO']=np.NAN\n",
        "data['R_CREDIT_TERM'] = round(data['CREDIT_TERM'])\n",
        "test['R_CREDIT_TERM'] = round(test['CREDIT_TERM'])\n",
        "data.loc[data['R_CREDIT_TERM']>38,'R_CREDIT_TERM']=38\n",
        "test.loc[test['R_CREDIT_TERM']>38,'R_CREDIT_TERM']=38\n",
        "data['F_CREDIT_TERM_10']=0\n",
        "test['F_CREDIT_TERM_10']=0\n",
        "data.loc[(data['NAME_CONTRACT_TYPE']!='Cash loans')&(data['R_CREDIT_TERM']==10.0),'F_CREDIT_TERM_10']=1\n",
        "test.loc[(test['NAME_CONTRACT_TYPE']!='Cash loans')&(test['R_CREDIT_TERM']==10.0),'F_CREDIT_TERM_10']=1\n",
        "data.loc[(data['NAME_CONTRACT_TYPE']!='Cash loans')&(data['R_CREDIT_TERM']==10.0),'R_CREDIT_TERM']=np.NAN\n",
        "test.loc[(test['NAME_CONTRACT_TYPE']!='Cash loans')&(test['R_CREDIT_TERM']==10.0),'R_CREDIT_TERM']=np.NAN\n",
        "data.loc[(data['NAME_CONTRACT_TYPE']!='Cash loans')&(data['R_CREDIT_TERM']==10.0),'CREDIT_TERM']=np.NAN\n",
        "test.loc[(test['NAME_CONTRACT_TYPE']!='Cash loans')&(test['R_CREDIT_TERM']==10.0),'CREDIT_TERM']=np.NAN\n",
        "data['F_CREDIT_TERM_20']=0\n",
        "test['F_CREDIT_TERM_20']=0\n",
        "data.loc[(data['NAME_CONTRACT_TYPE']!='Cash loans')&(data['R_CREDIT_TERM']==20.0),'F_CREDIT_TERM_20']=1\n",
        "test.loc[(test['NAME_CONTRACT_TYPE']!='Cash loans')&(test['R_CREDIT_TERM']==20.0),'F_CREDIT_TERM_20']=1\n",
        "data.loc[(data['NAME_CONTRACT_TYPE']!='Cash loans')&(data['R_CREDIT_TERM']==20.0),'R_CREDIT_TERM']=np.NAN\n",
        "test.loc[(test['NAME_CONTRACT_TYPE']!='Cash loans')&(test['R_CREDIT_TERM']==20.0),'R_CREDIT_TERM']=np.NAN\n",
        "data.loc[(data['NAME_CONTRACT_TYPE']!='Cash loans')&(data['R_CREDIT_TERM']==10.0),'CREDIT_TERM']=np.NAN\n",
        "test.loc[(test['NAME_CONTRACT_TYPE']!='Cash loans')&(test['R_CREDIT_TERM']==10.0),'CREDIT_TERM']=np.NAN\n",
        "data.drop(['NAME_CONTRACT_TYPE'],axis=1,inplace=True)\n",
        "test.drop(['NAME_CONTRACT_TYPE'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Cz6t407W1zzX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Separate target variable\n",
        "y = data['TARGET']\n",
        "del data['TARGET']\n",
        "\n",
        "#One-hot encoding of categorical features in data and test sets\n",
        "categorical_features = [col for col in data.columns if data[col].dtype == 'object']\n",
        "\n",
        "one_hot_df = pd.concat([data,test])\n",
        "one_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n",
        "\n",
        "data = one_hot_df.iloc[:data.shape[0],:]\n",
        "test = one_hot_df.iloc[data.shape[0]:,]"
      ],
      "metadata": {
        "id": "-cbgC9G8yy8-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Align data and test\n",
        "\n",
        "data_labels = y\n",
        "\n",
        "# Align the dataing and testing data, keep only columns present in both dataframes\n",
        "data, test = data.align(test, join = 'inner', axis = 1)\n",
        "\n",
        "# Add the target back in\n",
        "data['TARGET'] = y\n",
        "\n",
        "print('dataing Features shape: ', data.shape)\n",
        "print('Testing Features shape: ', test.shape)"
      ],
      "metadata": {
        "_uuid": "6e757d42434a84f5ea6f0db4504d0dec3ed77b09",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkf6FEVeonMu",
        "outputId": "88648f17-ea98-40a5-c61c-a789f38ad43e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataing Features shape:  (171202, 751)\n",
            "Testing Features shape:  (61500, 750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial Features\n",
        "\n",
        "# Make a new dataframe for polynomial features\n",
        "poly_features = data[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
        "poly_features_test = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3','DAYS_BIRTH']]\n",
        "\n",
        "# imputer for handling missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "poly_target = poly_features['TARGET']\n",
        "\n",
        "poly_features = poly_features.drop(columns = ['TARGET'])\n",
        "\n",
        "# Need to impute missing values\n",
        "poly_features = imputer.fit_transform(poly_features)\n",
        "poly_features_test = imputer.transform(poly_features_test)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "                                  \n",
        "# Create the polynomial object with specified degree\n",
        "poly_transformer = PolynomialFeatures(degree = 3)"
      ],
      "metadata": {
        "_uuid": "736bdd44bf134095f687e2e3b91196d56cc71c53",
        "trusted": true,
        "id": "Q9bMtZCTonMw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data the polynomial features\n",
        "poly_transformer.fit(poly_features)\n",
        "\n",
        "# Transform the features\n",
        "poly_features = poly_transformer.transform(poly_features)\n",
        "poly_features_test = poly_transformer.transform(poly_features_test)\n",
        "print('Polynomial Features shape: ', poly_features.shape)"
      ],
      "metadata": {
        "_uuid": "61ee5da3f1dbcb10867ff7c585745f3808e570e5",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-IaPc9eonMw",
        "outputId": "4696ef79-4433-4ab8-d4c6-a66bda017bee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Features shape:  (171202, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
      ],
      "metadata": {
        "_uuid": "c9b450d23bba070a887b0a4ed3bbb0d9f0e29499",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJjUa1VbonMx",
        "outputId": "ffb28bbb-2889-4a22-bf2b-4d109171abae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1',\n",
              " 'EXT_SOURCE_1',\n",
              " 'EXT_SOURCE_2',\n",
              " 'EXT_SOURCE_3',\n",
              " 'DAYS_BIRTH',\n",
              " 'EXT_SOURCE_1^2',\n",
              " 'EXT_SOURCE_1 EXT_SOURCE_2',\n",
              " 'EXT_SOURCE_1 EXT_SOURCE_3',\n",
              " 'EXT_SOURCE_1 DAYS_BIRTH',\n",
              " 'EXT_SOURCE_2^2',\n",
              " 'EXT_SOURCE_2 EXT_SOURCE_3',\n",
              " 'EXT_SOURCE_2 DAYS_BIRTH',\n",
              " 'EXT_SOURCE_3^2',\n",
              " 'EXT_SOURCE_3 DAYS_BIRTH',\n",
              " 'DAYS_BIRTH^2']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of the features \n",
        "poly_features = pd.DataFrame(poly_features, \n",
        "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
        "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Add in the target\n",
        "poly_features['TARGET'] = poly_target\n",
        "\n",
        "# Find the correlations with the target\n",
        "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display most negative and most positive\n",
        "print(poly_corrs.head(20))\n",
        "print(poly_corrs.tail(20))"
      ],
      "metadata": {
        "_uuid": "153207b73e2b7977487c84cc9260323e30f3561a",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo1ma7ZponMx",
        "outputId": "e689d852-2b15-413e-a0d3-4b8897ad4771"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXT_SOURCE_2 EXT_SOURCE_3                -0.190619\n",
            "EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3   -0.187364\n",
            "EXT_SOURCE_2^2 EXT_SOURCE_3              -0.174604\n",
            "EXT_SOURCE_2 EXT_SOURCE_3^2              -0.169516\n",
            "EXT_SOURCE_1 EXT_SOURCE_2                -0.165062\n",
            "EXT_SOURCE_2                             -0.162383\n",
            "EXT_SOURCE_1 EXT_SOURCE_2^2              -0.155854\n",
            "EXT_SOURCE_2^2                           -0.151487\n",
            "EXT_SOURCE_1 EXT_SOURCE_3                -0.150418\n",
            "EXT_SOURCE_3                             -0.142190\n",
            "EXT_SOURCE_2^3                           -0.142178\n",
            "EXT_SOURCE_1 EXT_SOURCE_3^2              -0.138980\n",
            "EXT_SOURCE_1^2 EXT_SOURCE_2              -0.138804\n",
            "EXT_SOURCE_2 DAYS_BIRTH^2                -0.135825\n",
            "EXT_SOURCE_3^2                           -0.129820\n",
            "EXT_SOURCE_1^2 EXT_SOURCE_3              -0.127705\n",
            "EXT_SOURCE_3 DAYS_BIRTH^2                -0.120932\n",
            "EXT_SOURCE_3^3                           -0.117608\n",
            "EXT_SOURCE_1 DAYS_BIRTH^2                -0.090729\n",
            "EXT_SOURCE_1                             -0.081994\n",
            "Name: TARGET, dtype: float64\n",
            "EXT_SOURCE_3 DAYS_BIRTH^2              -0.120932\n",
            "EXT_SOURCE_3^3                         -0.117608\n",
            "EXT_SOURCE_1 DAYS_BIRTH^2              -0.090729\n",
            "EXT_SOURCE_1                           -0.081994\n",
            "DAYS_BIRTH^2                           -0.077987\n",
            "EXT_SOURCE_1^2                         -0.075605\n",
            "EXT_SOURCE_1^3                         -0.068726\n",
            "DAYS_BIRTH^3                            0.075583\n",
            "DAYS_BIRTH                              0.079541\n",
            "EXT_SOURCE_1^2 DAYS_BIRTH               0.089520\n",
            "EXT_SOURCE_1 DAYS_BIRTH                 0.097990\n",
            "EXT_SOURCE_3^2 DAYS_BIRTH               0.133807\n",
            "EXT_SOURCE_3 DAYS_BIRTH                 0.141401\n",
            "EXT_SOURCE_1 EXT_SOURCE_3 DAYS_BIRTH    0.143557\n",
            "EXT_SOURCE_2^2 DAYS_BIRTH               0.152261\n",
            "EXT_SOURCE_1 EXT_SOURCE_2 DAYS_BIRTH    0.157203\n",
            "EXT_SOURCE_2 DAYS_BIRTH                 0.159863\n",
            "EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH    0.179790\n",
            "TARGET                                  1.000000\n",
            "1                                            NaN\n",
            "Name: TARGET, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put test features into dataframe\n",
        "poly_features_test = pd.DataFrame(poly_features_test, \n",
        "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
        "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Merge polynomial features into dataing dataframe\n",
        "poly_features['SK_ID_CURR'] = data['SK_ID_CURR']\n",
        "app_train_poly = data.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge polnomial features into testing dataframe\n",
        "poly_features_test['SK_ID_CURR'] = test['SK_ID_CURR']\n",
        "app_test_poly = test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Align the dataframes\n",
        "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
        "\n",
        "# Print out the new shapes\n",
        "print('dataing data with polynomial features shape: ', app_train_poly.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
      ],
      "metadata": {
        "_uuid": "9b871b3d3c3779694cc5a91e2871f1843f50ea10",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWg1TKruonMy",
        "outputId": "d75f8de4-975e-4cf3-81c0-0b01fa8b922a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataing data with polynomial features shape:  (171202, 785)\n",
            "Testing data with polynomial features shape:   (61500, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_poly.head()\n",
        "\n",
        "app_train_poly=app_train_poly.rename(columns={'EXT_SOURCE_1_x':'EXT_SOURCE_1','EXT_SOURCE_2_x':'EXT_SOURCE_2','EXT_SOURCE_3_x':'EXT_SOURCE_3','DAYS_BIRTH_x':'DAYS_BIRTH'})\n",
        "#app_train_poly=app_train_poly.drop('1',inplace=True)\n",
        "\n",
        "app_test_poly=app_test_poly.rename(columns={'EXT_SOURCE_1_x':'EXT_SOURCE_1','EXT_SOURCE_2_x':'EXT_SOURCE_2','EXT_SOURCE_3_x':'EXT_SOURCE_3','DAYS_BIRTH_x':'DAYS_BIRTH'})"
      ],
      "metadata": {
        "_uuid": "caa0f8362cb926895b0fdfcbaf4e10142bdd59aa",
        "trusted": true,
        "id": "Nx8q0Wt8onMy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_test_poly.drop('1',axis=1,inplace=True)\n",
        "app_train_poly.drop('1',axis=1,inplace=True)"
      ],
      "metadata": {
        "_uuid": "a310f5f00e12cf5caa074727275b1fa2a9bb8351",
        "trusted": true,
        "id": "I4WHW6wqonMy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_poly['NAME_TYPE_SUITE_Spouse, partner_x']"
      ],
      "metadata": {
        "_uuid": "eeb37d66185421105ef0d95ddd62de144ac273d6",
        "trusted": true,
        "id": "EeXj1CY4onMz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check and remove constant columns\n",
        "#colsToRemove = []\n",
        "#for col in app_train_poly.columns:\n",
        "#    if col != 'SK_ID_CURR' and col != 'TARGET':\n",
        "#        if app_train_poly[col].std() == 0: \n",
        "#            colsToRemove.append(col)\n",
        "        \n",
        "# remove constant columns in the training set\n",
        "#app_train_poly.drop(colsToRemove, axis=1, inplace=True)\n",
        "\n",
        "# remove constant columns in the test set\n",
        "#app_test_poly.drop(colsToRemove, axis=1, inplace=True) \n",
        "\n",
        "#print(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\n",
        "#print(colsToRemove)"
      ],
      "metadata": {
        "_uuid": "3385d341e3db4d3cd7c8f1d822a1eaa9f962a4d9",
        "trusted": true,
        "id": "TXWT6dREonMz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_domain['SK_ID_CURR']"
      ],
      "metadata": {
        "_uuid": "c6db4452ac6ec7ff57e558e9e84809b019d84e06",
        "trusted": true,
        "id": "0XnhH-PDonMz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create temp DF\n",
        "#data1 = pd.read_csv('../input/application_train.csv')\n",
        "#test1 = pd.read_csv('../input/application_test.csv')\n",
        "\n",
        "#app_train_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)\n",
        "#app_test_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)\n",
        "\n",
        "#app_train_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)\n",
        "#app_test_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)"
      ],
      "metadata": {
        "_uuid": "68e3df2199a9739da046815e73b6ab5e74014d41",
        "trusted": true,
        "id": "0yIeCNBOonM0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_poly['AMT_CREDIT'] = data1['AMT_CREDIT']\n",
        "#app_test_poly['AMT_CREDIT'] = test1['AMT_CREDIT']\n",
        "#app_train_poly['AMT_GOODS_PRICE'] = data1['AMT_GOODS_PRICE']\n",
        "#app_test_poly['AMT_GOODS_PRICE'] = test1['AMT_GOODS_PRICE']\n",
        "app_train_poly=app_train_poly.rename(columns={'AMT_CREDIT_x':'AMT_CREDIT','AMT_GOODS_PRICE_x':'AMT_GOODS_PRICE'})\n",
        "app_test_poly=app_test_poly.rename(columns={'AMT_CREDIT_x':'AMT_CREDIT','AMT_GOODS_PRICE_x':'AMT_GOODS_PRICE'})"
      ],
      "metadata": {
        "_uuid": "0e2e7d47f1cfb74e80fe0645cdf63f845ccfa24a",
        "trusted": true,
        "id": "EnrINXikonM0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_poly['AMT_ANNUITY_x']"
      ],
      "metadata": {
        "_uuid": "83924f6574e86c8afbc187448b21a96fd5b84957",
        "trusted": true,
        "id": "SwY10T5IonM1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain = app_train_poly.copy()\n",
        "app_test_domain = app_test_poly.copy()\n",
        "\n",
        "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n",
        "#app_train_domain['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_ANNUITY']\n",
        "app_train_domain['NEW_EXT_SOURCES_MEAN'] = app_train_domain[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "#\n",
        "app_train_domain['NEW_EMPLOY_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_EMPLOYED']\n",
        "app_train_domain['NEW_PHONE_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_REGISTRATION_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_REGISTRATION']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_PHONE_TO_REGISTRATION_NUM'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_EMPLOY_TO_REGISTRATION_NUM'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_EMPLOYED']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_REGISTRATION_NUM'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_PHONE_TO_EMPLOY_NUM'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_EMPLOY_NUM'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_REGION_RATING_CLIENT_NUM'] = app_train_domain['REGION_RATING_CLIENT_W_CITY'] - app_train_domain['REGION_RATING_CLIENT']\n",
        "app_train_domain['NEW_FAM_MEMBERS_NUM'] = app_train_domain['CNT_FAM_MEMBERS'] - app_train_domain['CNT_CHILDREN']\n",
        "app_train_domain['NEW_EMPLOY_TO_BIRTH_DAY'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_EMPLOYED']\n",
        "app_train_domain['NEW_PHONE_TO_BIRTH_DAY'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_REGISTRATION_TO_BIRTH_DAY'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_REGISTRATION']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_BIRTH_DAY'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_OWN_CAR_TO_BIRTH_DAY'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['OWN_CAR_AGE']\n",
        "app_train_domain['NEW_PHONE_TO_REGISTRATION_DAY'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_EMPLOY_TO_REGISTRATION_DAY'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_EMPLOYED']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_REGISTRATION_DAY'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_OWN_CAR_TO_REGISTRATION_DAY'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['OWN_CAR_AGE']\n",
        "app_train_domain['NEW_PHONE_TO_EMPLOY_DAY'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_EMPLOY_DAY'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_OWN_CAR_TO_EMPLOY_DAY'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['OWN_CAR_AGE']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_OWN_CAR'] = app_train_domain['OWN_CAR_AGE'] - app_train_domain['DAYS_ID_PUBLISH']"
      ],
      "metadata": {
        "_uuid": "6b77e86a1f80af0623a4bdc6572b1e7d64f868cb",
        "trusted": true,
        "id": "-37XbYmgonM1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']\n",
        "\n",
        "#app_test_domain['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_ANNUITY']\n",
        "app_test_domain['NEW_EXT_SOURCES_MEAN'] = app_test_domain[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "#\n",
        "app_test_domain['NEW_EMPLOY_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_EMPLOYED']\n",
        "app_test_domain['NEW_PHONE_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_REGISTRATION_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_REGISTRATION']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_PHONE_TO_REGISTRATION_NUM'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_EMPLOY_TO_REGISTRATION_NUM'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_EMPLOYED']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_REGISTRATION_NUM'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_PHONE_TO_EMPLOY_NUM'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_EMPLOY_NUM'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_REGION_RATING_CLIENT_NUM'] = app_test_domain['REGION_RATING_CLIENT_W_CITY'] - app_test_domain['REGION_RATING_CLIENT']\n",
        "app_test_domain['NEW_FAM_MEMBERS_NUM'] = app_test_domain['CNT_FAM_MEMBERS'] - app_test_domain['CNT_CHILDREN']\n",
        "app_test_domain['NEW_EMPLOY_TO_BIRTH_DAY'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_EMPLOYED']\n",
        "app_test_domain['NEW_PHONE_TO_BIRTH_DAY'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_REGISTRATION_TO_BIRTH_DAY'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_REGISTRATION']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_BIRTH_DAY'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_OWN_CAR_TO_BIRTH_DAY'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['OWN_CAR_AGE']\n",
        "app_test_domain['NEW_PHONE_TO_REGISTRATION_DAY'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_EMPLOY_TO_REGISTRATION_DAY'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_EMPLOYED']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_REGISTRATION_DAY'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_OWN_CAR_TO_REGISTRATION_DAY'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['OWN_CAR_AGE']\n",
        "app_test_domain['NEW_PHONE_TO_EMPLOY_DAY'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_EMPLOY_DAY'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_OWN_CAR_TO_EMPLOY_DAY'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['OWN_CAR_AGE']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_OWN_CAR'] = app_test_domain['OWN_CAR_AGE'] - app_test_domain['DAYS_ID_PUBLISH']"
      ],
      "metadata": {
        "_uuid": "4505cd992232d3158315214fedfd525c4c9977e9",
        "trusted": true,
        "id": "YK6YfTJJonM1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain['TARGET'] = poly_target\n",
        "print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_domain.shape)"
      ],
      "metadata": {
        "_uuid": "5ea0b3d3ee19c8adfb307fd13f53e02f018be762",
        "trusted": true,
        "id": "2HYOhQeFonM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7934e83-17c9-4e05-ca28-89142c3e0235"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data with polynomial features shape:  (171202, 811)\n",
            "Testing data with polynomial features shape:   (61500, 810)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_domain = app_train_domain[app_train_domain.columns[app_train_domain.isnull().mean() < 0.80]]\n",
        "#app_test_domain = app_test_domain[app_test_domain.columns[app_test_domain.isnull().mean() < 0.80]]\n",
        "#print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "#print('Testing data with polynomial features shape:  ', app_test_domain.shape)"
      ],
      "metadata": {
        "_uuid": "336e903a7a4c4be26bc3cc4943cd11c243970c02",
        "trusted": true,
        "id": "b_eUsThqonM2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_domain = app_test_domain.drop('AMT_ANNUITY',axis=1)\n",
        "#app_test_domain = app_test_domain.drop('AMT_ANNUITY',axis=1)"
      ],
      "metadata": {
        "_uuid": "d6d064e084efb6373abd92c4961fab4552b02d71",
        "trusted": true,
        "id": "CWyvuokconM2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_domain['TARGET']"
      ],
      "metadata": {
        "_uuid": "f293952bbffde2a8b4e6ad4524cf9de08eb8e6dd",
        "trusted": true,
        "id": "xwfRLeVoonM2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain['TARGET'] = poly_target\n",
        "print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_domain.shape)"
      ],
      "metadata": {
        "_uuid": "f4621c5cdf5ed89f3a3806f689e93c64bd1d4fae",
        "trusted": true,
        "id": "Pf7NVRztonM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9f6c63-38ea-4a9c-e63f-a06a4cf9b153"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data with polynomial features shape:  (171202, 811)\n",
            "Testing data with polynomial features shape:   (61500, 810)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_domain = app_train_domain.reindex(\n",
        " #   np.random.permutation(app_train_domain.index))"
      ],
      "metadata": {
        "_uuid": "b50311403d93e99586819404a60997fd815db0cf",
        "trusted": true,
        "id": "cHKp00mAonM3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#app_train_domain=app_train_domain.drop('TARGET',axis=1)\n",
        "#app_train_domain = np.log1p(app_train_domain)\n",
        "#app_test_domain=np.log1p(app_test_domain)\n",
        "#print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "#print('Testing data with polynomial features shape:  ', app_test_domain.shape)"
      ],
      "metadata": {
        "_uuid": "70d8c21e78526c445819b3f78647ab3bcd0615de",
        "trusted": true,
        "id": "vann9ARGonM3"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "\n",
        "def model(features, test_features, encoding = 'ohe', n_folds =5 ):\n",
        "    \n",
        "    \"\"\"Train and test a light gradient boosting model using\n",
        "    cross validation. \n",
        "    \n",
        "    Parameters\n",
        "    --------\n",
        "        features (pd.DataFrame): \n",
        "            dataframe of training features to use \n",
        "            for training a model. Must include the TARGET column.\n",
        "        test_features (pd.DataFrame): \n",
        "            dataframe of testing features to use\n",
        "            for making predictions with the model. \n",
        "        encoding (str, default = 'ohe'): \n",
        "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
        "            n_folds (int, default = 5): number of folds to use for cross validation\n",
        "        \n",
        "    Return\n",
        "    --------\n",
        "        submission (pd.DataFrame): \n",
        "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
        "            predicted by the model.\n",
        "        feature_importances (pd.DataFrame): \n",
        "            dataframe with the feature importances from the model.\n",
        "        valid_metrics (pd.DataFrame): \n",
        "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract the ids\n",
        "    train_ids = features['SK_ID_CURR']\n",
        "    test_ids = test_features['SK_ID_CURR']\n",
        "    \n",
        "    # Extract the labels for training\n",
        "    labels = features['TARGET']\n",
        "    \n",
        "    # Remove the ids and target\n",
        "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
        "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
        "    \n",
        "    \n",
        "    # One Hot Encoding\n",
        "    if encoding == 'ohe':\n",
        "        features = pd.get_dummies(features)\n",
        "        test_features = pd.get_dummies(test_features)\n",
        "        \n",
        "        # Align the dataframes by the columns\n",
        "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
        "        \n",
        "        # No categorical indices to record\n",
        "        cat_indices = 'auto'\n",
        "    \n",
        "    # Integer label encoding\n",
        "    elif encoding == 'le':\n",
        "        \n",
        "        # Create a label encoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        \n",
        "        # List for storing categorical indices\n",
        "        cat_indices = []\n",
        "        \n",
        "        # Iterate through each column\n",
        "        for i, col in enumerate(features):\n",
        "            if features[col].dtype == 'object':\n",
        "                # Map the categorical features to integers\n",
        "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
        "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
        "\n",
        "                # Record the categorical indices\n",
        "                cat_indices.append(i)\n",
        "    \n",
        "    # Catch error if label encoding scheme is not valid\n",
        "    else:\n",
        "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
        "        \n",
        "    print('Training Data Shape: ', features.shape)\n",
        "    print('Testing Data Shape: ', test_features.shape)\n",
        "    \n",
        "    # Extract feature names\n",
        "    feature_names = list(features.columns)\n",
        "    \n",
        "    # Convert to np arrays\n",
        "    features = np.array(features)\n",
        "    test_features = np.array(test_features)\n",
        "    \n",
        "    # Create the kfold object\n",
        "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
        "    \n",
        "    # Empty array for feature importances\n",
        "    feature_importance_values = np.zeros(len(feature_names))\n",
        "    \n",
        "    # Empty array for test predictions\n",
        "    test_predictions = np.zeros(test_features.shape[0])\n",
        "    \n",
        "    # Empty array for out of fold validation predictions\n",
        "    out_of_fold = np.zeros(features.shape[0])\n",
        "    \n",
        "    # Lists for recording validation and training scores\n",
        "    valid_scores = []\n",
        "    train_scores = []\n",
        "    \n",
        "    # Iterate through each fold\n",
        "    for train_indices, valid_indices in k_fold.split(features):\n",
        "        \n",
        "        # Training data for the fold\n",
        "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
        "        # Validation data for the fold\n",
        "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
        "        \n",
        "        # Create the model\n",
        "        model = lgb.LGBMClassifier(n_estimators=10000, nthread=4,objective = 'binary', \n",
        "                                   class_weight = 'balanced', learning_rate = 0.015, \n",
        "                                   reg_alpha = 0.041545473, reg_lambda = 0.1, \n",
        "                                   n_jobs = -1, random_state = 50,num_leaves=32,colsample_bytree=.9497036,subsample=.8715623,\n",
        "                                  max_depth=5,min_split_gain=.0222415,min_child_weight=39.3259775,max_bin=200,num_boost_round=3000,min_data_in_leaf=100,bagging_fraction=0.5,bagging_freq=10)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
        "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
        "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
        "                  early_stopping_rounds = 100, verbose = 200)\n",
        "        \n",
        "        # Record the best iteration\n",
        "        best_iteration = model.best_iteration_\n",
        "        \n",
        "        # Record the feature importances\n",
        "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
        "        \n",
        "        # Make predictions\n",
        "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
        "        \n",
        "        # Record the out of fold predictions\n",
        "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
        "        \n",
        "        # Record the best score\n",
        "        valid_score = model.best_score_['valid']['auc']\n",
        "        train_score = model.best_score_['train']['auc']\n",
        "        \n",
        "        valid_scores.append(valid_score)\n",
        "        train_scores.append(train_score)\n",
        "        \n",
        "        # Clean up memory\n",
        "        gc.enable()\n",
        "        del model, train_features, valid_features\n",
        "        gc.collect()\n",
        "        \n",
        "    # Make the submission dataframe\n",
        "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
        "    \n",
        "    # Make the feature importance dataframe\n",
        "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
        "    \n",
        "    # Overall validation score\n",
        "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
        "    \n",
        "    # Add the overall scores to the metrics\n",
        "    valid_scores.append(valid_auc)\n",
        "    train_scores.append(np.mean(train_scores))\n",
        "    \n",
        "    # Needed for creating dataframe of validation scores\n",
        "    fold_names = list(range(n_folds))\n",
        "    fold_names.append('overall')\n",
        "    \n",
        "    # Dataframe of validation scores\n",
        "    metrics = pd.DataFrame({'fold': fold_names,\n",
        "                            'train': train_scores,\n",
        "                            'valid': valid_scores}) \n",
        "    \n",
        "    return submission, feature_importances, metrics"
      ],
      "metadata": {
        "_uuid": "b5b517cbd8769326456ce8936659088c2445859b",
        "trusted": true,
        "id": "W9WgYpa-onM4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train_domain_f10 = app_train_domain.loc[app_train_domain['F_CREDIT_TERM_10']==1,:].reset_index(drop=True)\n",
        "app_test_domain_f10 = app_test_domain.loc[app_test_domain['F_CREDIT_TERM_10']==1,:].reset_index(drop=True)\n",
        "app_train_domain_f20 = app_train_domain.loc[app_train_domain['F_CREDIT_TERM_20']==1,:].reset_index(drop=True)\n",
        "app_test_domain_f20 = app_test_domain.loc[app_test_domain['F_CREDIT_TERM_20']==1,:].reset_index(drop=True)\n",
        "app_train_domain = app_train_domain.loc[(app_train_domain['F_CREDIT_TERM_10']==0)&\n",
        "                                        (app_train_domain['F_CREDIT_TERM_20']==0),:].reset_index(drop=True)\n",
        "app_test_domain = app_test_domain.loc[(app_test_domain['F_CREDIT_TERM_10']==0)&\n",
        "                                        (app_test_domain['F_CREDIT_TERM_20']==0),:].reset_index(drop=True)\n",
        "app_train_domain.drop(['F_CREDIT_TERM_10','F_CREDIT_TERM_20'],axis=1,inplace=True)\n",
        "app_test_domain.drop(['F_CREDIT_TERM_10','F_CREDIT_TERM_20'],axis=1,inplace=True)\n",
        "app_train_domain_f10.drop(['F_CREDIT_TERM_10','F_CREDIT_TERM_20'],axis=1,inplace=True)\n",
        "app_test_domain_f10.drop(['F_CREDIT_TERM_10','F_CREDIT_TERM_20'],axis=1,inplace=True)\n",
        "app_train_domain_f20.drop(['F_CREDIT_TERM_10','F_CREDIT_TERM_20'],axis=1,inplace=True)\n",
        "app_test_domain_f20.drop(['F_CREDIT_TERM_10','F_CREDIT_TERM_20'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "V4L9q9dr-jm_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission, fi, metrics = model(app_train_domain, app_test_domain)\n",
        "print('Baseline metrics')\n",
        "print(metrics)"
      ],
      "metadata": {
        "_uuid": "a1c8b9a78defac7e48f7188ce2b0dc851c1b5788",
        "trusted": true,
        "id": "-yJV0Cm9onM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374a3cda-5e17-4eea-e70b-7e56fc07e383"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shape:  (154988, 807)\n",
            "Testing Data Shape:  (55794, 807)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.767766\ttrain's binary_logloss: 0.580453\tvalid's auc: 0.749901\tvalid's binary_logloss: 0.583242\n",
            "[400]\ttrain's auc: 0.7849\ttrain's binary_logloss: 0.561542\tvalid's auc: 0.755491\tvalid's binary_logloss: 0.568747\n",
            "[600]\ttrain's auc: 0.795668\ttrain's binary_logloss: 0.550199\tvalid's auc: 0.757124\tvalid's binary_logloss: 0.56422\n",
            "[800]\ttrain's auc: 0.807391\ttrain's binary_logloss: 0.538542\tvalid's auc: 0.758947\tvalid's binary_logloss: 0.556884\n",
            "Early stopping, best iteration is:\n",
            "[870]\ttrain's auc: 0.810476\ttrain's binary_logloss: 0.535266\tvalid's auc: 0.759473\tvalid's binary_logloss: 0.553993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.767567\ttrain's binary_logloss: 0.580235\tvalid's auc: 0.741379\tvalid's binary_logloss: 0.582313\n",
            "[400]\ttrain's auc: 0.784879\ttrain's binary_logloss: 0.560833\tvalid's auc: 0.747956\tvalid's binary_logloss: 0.570818\n",
            "[600]\ttrain's auc: 0.795591\ttrain's binary_logloss: 0.549956\tvalid's auc: 0.750263\tvalid's binary_logloss: 0.564535\n",
            "[800]\ttrain's auc: 0.805658\ttrain's binary_logloss: 0.53979\tvalid's auc: 0.751558\tvalid's binary_logloss: 0.556698\n",
            "Early stopping, best iteration is:\n",
            "[739]\ttrain's auc: 0.802844\ttrain's binary_logloss: 0.542736\tvalid's auc: 0.7519\tvalid's binary_logloss: 0.558285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.766293\ttrain's binary_logloss: 0.581727\tvalid's auc: 0.752194\tvalid's binary_logloss: 0.58044\n",
            "[400]\ttrain's auc: 0.784597\ttrain's binary_logloss: 0.561908\tvalid's auc: 0.761143\tvalid's binary_logloss: 0.565382\n",
            "[600]\ttrain's auc: 0.796329\ttrain's binary_logloss: 0.550259\tvalid's auc: 0.762061\tvalid's binary_logloss: 0.557883\n",
            "Early stopping, best iteration is:\n",
            "[650]\ttrain's auc: 0.798895\ttrain's binary_logloss: 0.547723\tvalid's auc: 0.762586\tvalid's binary_logloss: 0.555282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.769246\ttrain's binary_logloss: 0.579092\tvalid's auc: 0.746763\tvalid's binary_logloss: 0.584548\n",
            "[400]\ttrain's auc: 0.78599\ttrain's binary_logloss: 0.560297\tvalid's auc: 0.751719\tvalid's binary_logloss: 0.573544\n",
            "[600]\ttrain's auc: 0.797114\ttrain's binary_logloss: 0.54868\tvalid's auc: 0.753587\tvalid's binary_logloss: 0.566215\n",
            "Early stopping, best iteration is:\n",
            "[583]\ttrain's auc: 0.796367\ttrain's binary_logloss: 0.549348\tvalid's auc: 0.753629\tvalid's binary_logloss: 0.567594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.768471\ttrain's binary_logloss: 0.579757\tvalid's auc: 0.743527\tvalid's binary_logloss: 0.585069\n",
            "[400]\ttrain's auc: 0.784318\ttrain's binary_logloss: 0.562016\tvalid's auc: 0.749135\tvalid's binary_logloss: 0.573946\n",
            "[600]\ttrain's auc: 0.796081\ttrain's binary_logloss: 0.54989\tvalid's auc: 0.752694\tvalid's binary_logloss: 0.565182\n",
            "[800]\ttrain's auc: 0.804552\ttrain's binary_logloss: 0.541136\tvalid's auc: 0.754094\tvalid's binary_logloss: 0.559317\n",
            "[1000]\ttrain's auc: 0.812666\ttrain's binary_logloss: 0.532798\tvalid's auc: 0.754569\tvalid's binary_logloss: 0.555422\n",
            "Early stopping, best iteration is:\n",
            "[1051]\ttrain's auc: 0.814595\ttrain's binary_logloss: 0.530659\tvalid's auc: 0.754663\tvalid's binary_logloss: 0.554619\n",
            "Baseline metrics\n",
            "      fold     train     valid\n",
            "0        0  0.810476  0.759473\n",
            "1        1  0.802844  0.751900\n",
            "2        2  0.798895  0.762586\n",
            "3        3  0.796367  0.753629\n",
            "4        4  0.814595  0.754663\n",
            "5  overall  0.804636  0.756391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "base 0.76232\n",
        "\n",
        "Baseline metrics\n",
        "      fold     train     valid\n",
        "*        0  0.810763  0.759339\n",
        "*        1  0.816707  0.754015\n",
        "*        2  0.798839  0.763283\n",
        "*        3  0.795580  0.754167\n",
        "*        4  0.817558  0.755688\n",
        "*  overall  0.807890  0.757188"
      ],
      "metadata": {
        "id": "lxaVrnbTh3N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fi_sorted = plot_feature_importances(fi)"
      ],
      "metadata": {
        "_uuid": "8a2d7f50ba5347ab337d851f3a5a5cfe765e098f",
        "trusted": true,
        "id": "yT__4iaXonM5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_f20, fi_f20, metrics_f20 = model(app_train_domain_f20, app_test_domain_f20)\n",
        "print('Baseline metrics')\n",
        "print(metrics_f20)\n"
      ],
      "metadata": {
        "id": "eD47ckWYAlWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441de418-265b-4032-b66a-561275003068"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shape:  (16188, 807)\n",
            "Testing Data Shape:  (5701, 807)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttrain's auc: 0.794385\ttrain's binary_logloss: 0.589962\tvalid's auc: 0.774014\tvalid's binary_logloss: 0.579863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.824575\ttrain's binary_logloss: 0.525549\tvalid's auc: 0.738985\tvalid's binary_logloss: 0.522119\n",
            "Early stopping, best iteration is:\n",
            "[280]\ttrain's auc: 0.840393\ttrain's binary_logloss: 0.504835\tvalid's auc: 0.743319\tvalid's binary_logloss: 0.509375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.836272\ttrain's binary_logloss: 0.514843\tvalid's auc: 0.733175\tvalid's binary_logloss: 0.523658\n",
            "Early stopping, best iteration is:\n",
            "[231]\ttrain's auc: 0.840999\ttrain's binary_logloss: 0.507197\tvalid's auc: 0.734648\tvalid's binary_logloss: 0.526397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.828896\ttrain's binary_logloss: 0.518753\tvalid's auc: 0.725606\tvalid's binary_logloss: 0.538173\n",
            "Early stopping, best iteration is:\n",
            "[239]\ttrain's auc: 0.837248\ttrain's binary_logloss: 0.508002\tvalid's auc: 0.728084\tvalid's binary_logloss: 0.529096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.835573\ttrain's binary_logloss: 0.513645\tvalid's auc: 0.73009\tvalid's binary_logloss: 0.529879\n",
            "Early stopping, best iteration is:\n",
            "[216]\ttrain's auc: 0.838809\ttrain's binary_logloss: 0.50903\tvalid's auc: 0.732676\tvalid's binary_logloss: 0.523192\n",
            "Baseline metrics\n",
            "      fold     train     valid\n",
            "0        0  0.794385  0.774014\n",
            "1        1  0.840393  0.743319\n",
            "2        2  0.840999  0.734648\n",
            "3        3  0.837248  0.728084\n",
            "4        4  0.838809  0.732676\n",
            "5  overall  0.830367  0.735324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "      fold     train     valid\n",
        "*        0  0.794437  0.774890\n",
        "*        1  0.802098  0.739384\n",
        "*        2  0.843865  0.733055\n",
        "*        3  0.835427  0.729012\n",
        "*        4  0.852630  0.731005\n",
        "*  overall  0.825691  0.732939"
      ],
      "metadata": {
        "id": "56j7MRuMVGpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fi_f20_sorted = plot_feature_importances(fi_f20)"
      ],
      "metadata": {
        "id": "yq9fQZZ1BIId"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "リボルビングローンのCredit term=10はデフォルト率ゼロ"
      ],
      "metadata": {
        "id": "-l3qfgj2CrL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_f10 = pd.DataFrame({'SK_ID_CURR': app_test_domain_f10['SK_ID_CURR']})\n",
        "submission_f10['TARGET']=0"
      ],
      "metadata": {
        "_uuid": "6ac3c002d26190d507f2634a11feb98d0569d6f6",
        "trusted": true,
        "id": "LSQ2ifqaonM5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_f10"
      ],
      "metadata": {
        "id": "hW9urLVLW87z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "82600774-987d-4813-96b2-d1a1a335cb49"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SK_ID_CURR  TARGET\n",
              "0      174613       0\n",
              "1      196810       0\n",
              "2      214869       0\n",
              "3      225806       0\n",
              "4      229681       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7151ab4b-cadc-4ba3-8181-96ee7b7f529b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SK_ID_CURR</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>174613</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>196810</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>214869</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>225806</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>229681</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7151ab4b-cadc-4ba3-8181-96ee7b7f529b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7151ab4b-cadc-4ba3-8181-96ee7b7f529b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7151ab4b-cadc-4ba3-8181-96ee7b7f529b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_result=pd.concat([submission,submission_f20,submission_f10],axis=0)\n",
        "submission_result.sort_values(by='SK_ID_CURR',inplace=True)\n"
      ],
      "metadata": {
        "_uuid": "d7d65820dfa0782f770ff075410320167718de27",
        "trusted": true,
        "id": "_TL9vP7oonM5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_result['TARGET'].hist(bins=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "MUDX-TsPELCZ",
        "outputId": "90542c8e-f84e-45e6-d12f-2af8ce491c66"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe40d5dbc50>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=0.35\n",
        "submission_result.loc[submission_result['TARGET']<p,'TARGET']=0\n",
        "submission_result.loc[submission_result['TARGET']>1-p,'TARGET']=1"
      ],
      "metadata": {
        "id": "wgXiRvwRFHxX"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_result['TARGET'].hist(bins=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qs3VrBhrFsAO",
        "outputId": "ddfb3279-d501-4142-8e1a-fe4d864fdb41"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe419871910>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_result.to_csv('lower_sub.csv', index = False)"
      ],
      "metadata": {
        "id": "Tothb38njf8X"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"June28_1930_0_76315.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1UCEsc9CPQGjrpyvJpzQtujQO2wgiUIFd\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print('Importing data...')\n",
        "# Google Colaboratoryで作業する場合はこちらも実行してください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# %cd 以降にこのnotebookを置いているディレクトリを指定してください。\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/GCI/02.（公開）コンペ2-20220621T094535Z-001.zip (Unzipped Files)/02.（公開）コンペ2/input/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/GCI/02.（公開）コンペ2-20220621T094535Z-001.zip (Unzipped Files)/02.（公開）コンペ2/input/test.csv\")\n",
        "\n",
        "#test = test[test.columns[data.isnull().mean() < 0.85]]\n",
        "#data = data[data.columns[data.isnull().mean() < 0.85]]\n",
        "\n",
        "data['FLAG_NOT_EMPLOYED']=0\n",
        "data.loc[data['DAYS_EMPLOYED']==365243,'FLAG_NOT_EMPLOYED']=1\n",
        "#data.loc[data['DAYS_EMPLOYED']==365243,'DAYS_EMPLOYED']=np.NAN\n",
        "test['FLAG_NOT_EMPLOYED']=0\n",
        "test.loc[test['DAYS_EMPLOYED']==365243,'FLAG_NOT_EMPLOYED']=1\n",
        "#test.loc[test['DAYS_EMPLOYED']==365243,'DAYS_EMPLOYED']=np.NAN\n",
        "\n",
        "cols =list(test.columns)\n",
        "cols\n",
        "\n",
        "\n",
        "\n",
        "data['YEARS_BIRTH']=round(data['DAYS_BIRTH']/365)\n",
        "test['YEARS_BIRTH']=round(test['DAYS_BIRTH']/365)\n",
        "\n",
        "data['YEARS_ID_PUBLISH']=round(data['DAYS_ID_PUBLISH']/365)\n",
        "test['YEARS_ID_PUBLISH']=round(test['DAYS_ID_PUBLISH']/365)\n",
        "\n",
        "data['YEARS_REGISTRATION']=round(data['DAYS_REGISTRATION']/365)\n",
        "test['YEARS_REGISTRATION']=round(test['DAYS_REGISTRATION']/365)\n",
        "\n",
        "\n",
        "data['YEARS_EMPLOYED']=round(data['DAYS_EMPLOYED']/365)\n",
        "test['YEARS_EMPLOYED']=round(test['DAYS_EMPLOYED']/365)\n",
        "\n",
        "\n",
        "\n",
        "dict={}\n",
        "for item in data['AMT_INCOME_TOTAL'].unique():\n",
        "  dict[item]=data.loc[data['AMT_INCOME_TOTAL']==item,'SK_ID_CURR'].count()\n",
        "df = pd.DataFrame.from_dict(dict,orient='index')\n",
        "\n",
        "items = list(df[df[0]>10])\n",
        "\n",
        "data['AMT_INCOME_TOTAL_2']=np.NAN\n",
        "test['AMT_INCOME_TOTAL_2']=np.NAN\n",
        "for item in items:\n",
        "  data.loc[data['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL_2']=data.loc[data['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL']\n",
        "  test.loc[test['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL_2']=test.loc[test['AMT_INCOME_TOTAL']==item,'AMT_INCOME_TOTAL']\n",
        "\n",
        "data=pd.get_dummies(data,columns=['AMT_INCOME_TOTAL_2'])\n",
        "test=pd.get_dummies(test,columns={'AMT_INCOME_TOTAL_2'})\n",
        "\n",
        "data.loc[data['REGION_POPULATION_RELATIVE']>0.04,'REGION_POPULATION_RELATIVE'].unique()\n",
        "\n",
        "data['REGION_POPULATION_RELATIVE_0.04622']=0\n",
        "data['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE_0.04622']=1\n",
        "data['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "test['REGION_POPULATION_RELATIVE_0.04622']=0\n",
        "test['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE_0.04622']=1\n",
        "test['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "data['REGION_POPULATION_RELATIVE_0.072508']=0\n",
        "data['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE_0.072508']=1\n",
        "data['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "test['REGION_POPULATION_RELATIVE_0.072508']=0\n",
        "test['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE_0.072508']=1\n",
        "test['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "data['OWN_CAR_AGE_64']=0\n",
        "data['OWN_CAR_AGE'==64,'OWN_CAR_AGE_64']=1\n",
        "data['OWN_CAR_AGE'==64,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "test['OWN_CAR_AGE_64']=0\n",
        "test['OWN_CAR_AGE'==64,'OWN_CAR_AGE_64']=1\n",
        "test['OWN_CAR_AGE'==64,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "data['OWN_CAR_AGE_65']=0\n",
        "data['OWN_CAR_AGE'==65,'OWN_CAR_AGE_65']=1\n",
        "data['OWN_CAR_AGE'==65,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "test['OWN_CAR_AGE_65']=0\n",
        "test['OWN_CAR_AGE'==65,'OWN_CAR_AGE_65']=1\n",
        "test['OWN_CAR_AGE'==65,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "data['LOW_DEFAULT_ORG']=0\n",
        "data.loc[data['ORGANIZATION_TYPE']=='Industry: type 12','LOW_DEFAULT_ORG' ]=1\n",
        "data.loc[data['ORGANIZATION_TYPE']=='Trade: type 4','LOW_DEFAULT_ORG' ]=1\n",
        "test['LOW_DEFAULT_ORG']=0\n",
        "test.loc[test['ORGANIZATION_TYPE']=='Industry: type 12','LOW_DEFAULT_ORG' ]=1\n",
        "test.loc[test['ORGANIZATION_TYPE']=='Trade: type 4','LOW_DEFAULT_ORG' ]=1\n",
        "\n",
        "data['HIGH_DEFAULT_ORG']=0\n",
        "data.loc[data['ORGANIZATION_TYPE']=='Transport: type 3','HIGH_DEFAULT_ORG']=1\n",
        "test['HIGH_DEFAULT_ORG']=0\n",
        "test.loc[test['ORGANIZATION_TYPE']=='Transport: type 3','HIGH_DEFAULT_ORG']=1\n",
        "\n",
        "# 欠測値であれば'_NAN'を加えたカラムを作る\n",
        "def flag_isNan(column_target):\n",
        "  if type(column_target)==str:\n",
        "    data[column_target+'_NAN']=0\n",
        "    test[column_target+'_NAN']=0\n",
        "    #\n",
        "    data.loc[data[column_target].isna(),column_target+'_NAN']=1\n",
        "    test.loc[test[column_target].isna(),column_target+'_NAN']=1\n",
        "\n",
        "list(test.dtypes[data.dtypes=='float'].keys())\n",
        "\n",
        "for col in ['AMT_INCOME_TOTAL',\n",
        " 'AMT_CREDIT',\n",
        " 'AMT_ANNUITY',\n",
        " 'AMT_GOODS_PRICE',\n",
        " 'REGION_POPULATION_RELATIVE',\n",
        " 'DAYS_REGISTRATION',\n",
        " 'OWN_CAR_AGE',\n",
        " 'CNT_FAM_MEMBERS',\n",
        " 'EXT_SOURCE_1',\n",
        " 'EXT_SOURCE_2',\n",
        " 'EXT_SOURCE_3',\n",
        " 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
        " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
        " 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
        " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
        " 'DAYS_LAST_PHONE_CHANGE',\n",
        " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
        " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
        " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
        " 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
        " 'YEARS_BIRTH',\n",
        " 'YEARS_ID_PUBLISH',\n",
        " 'YEARS_REGISTRATION',\n",
        " 'YEARS_EMPLOYED',\n",
        " #(False, 'REGION_POPULATION_RELATIVE'),\n",
        " #(False, 'OWN_CAR_AGE')\n",
        " ]:\n",
        "  if (test.loc[test[col].isna(),'SK_ID_CURR'].count()>0):\n",
        "    flag_isNan(col)\n",
        "\n",
        "#for col in list(test.dtypes[data.dtypes!='float'].keys()):\n",
        "#  if (test.loc[test[col].isna(),'SK_ID_CURR'].count()>0):\n",
        "#    flag_isNan(col)\n",
        "\n",
        "data['RT_CREDIT']=data['AMT_CREDIT']/data['AMT_INCOME_TOTAL']\n",
        "test['RT_CREDIT']=test['AMT_CREDIT']/test['AMT_INCOME_TOTAL']\n",
        "\n",
        "#data['FLAG_RT_CREDIT_OVER1']=0\n",
        "#test['FLAG_RT_CREDIT_OVER1']=0\n",
        "#data.loc[data['RT_CREDIT']>1,'FLAG_RT_CREDIT_OVER1']=1\n",
        "#test.loc[data['RT_CREDIT']>1,'FLAG_RT_CREDIT_OVER1']=1\n",
        "\n",
        "data['ROUND_RT_CREDIT']=round(data['RT_CREDIT'])\n",
        "data['ROUND_RT_CREDIT'].fillna(0,inplace=True)\n",
        "data.loc[data['ROUND_RT_CREDIT']>6,'ROUND_RT_CREDIT']=6\n",
        "data['ROUND_RT_CREDIT']=data['ROUND_RT_CREDIT'].astype(str)\n",
        "\n",
        "data=pd.get_dummies(data,columns=['ROUND_RT_CREDIT'])\n",
        "\n",
        "test['ROUND_RT_CREDIT']=round(test['RT_CREDIT'])\n",
        "test['ROUND_RT_CREDIT'].fillna(0,inplace=True)\n",
        "test.loc[test['ROUND_RT_CREDIT']>6,'ROUND_RT_CREDIT']=6\n",
        "test['ROUND_RT_CREDIT']=test['ROUND_RT_CREDIT'].astype(str)\n",
        "\n",
        "test=pd.get_dummies(test,columns=['ROUND_RT_CREDIT'])\n",
        "\n",
        "data['AGE_EMP']=data['YEARS_BIRTH']-data['YEARS_EMPLOYED']\n",
        "test['AGE_EMP']=test['YEARS_BIRTH']-test['YEARS_EMPLOYED']\n",
        "\n",
        "# 26歳までの上昇局面とそれ以降の下降局面を別のカラムにする\n",
        "data['AGE_EMP1']=np.NAN\n",
        "test['AGE_EMP1']=np.NAN\n",
        "data.loc[data['AGE_EMP']<-26,'AGE_EMP1']=data['AGE_EMP']\n",
        "test.loc[test['AGE_EMP']<-26,'AGE_EMP1']=test['AGE_EMP']\n",
        "data.loc[data['AGE_EMP']<-26,'AGE_EMP']=np.NAN\n",
        "test.loc[test['AGE_EMP']<-26,'AGE_EMP']=np.NAN\n",
        "\n",
        "#data['TMP_FLAG_OWN_REALTY']=data['FLAG_OWN_REALTY']\n",
        "#data.loc[data['TMP_FLAG_OWN_REALTY'].isna(),'TMP_FLAG_OWN_REALTY']='O'\n",
        "#data['ROUND_AMT_INCOME_TOTAL']=round(data['AMT_INCOME_TOTAL']/100000)\n",
        "#data.loc[data['ROUND_AMT_INCOME_TOTAL']>6,'ROUND_AMT_INCOME_TOTAL']=6\n",
        "#data['ROUND_AMT_INCOME_TOTAL']=data['ROUND_AMT_INCOME_TOTAL'].astype(str)\n",
        "#data['ROUND_RT_CREDIT']=round(data['RT_CREDIT'])\n",
        "#data.loc[data['ROUND_RT_CREDIT']>6,'ROUND_RT_CREDIT']=6\n",
        "#data['ROUND_RT_CREDIT'].fillna(0,inplace=True)\n",
        "#data['ROUND_RT_CREDIT']=data['ROUND_RT_CREDIT'].astype(str)\n",
        "#data['REALTY_INCOME_CREDIT']=data['TMP_FLAG_OWN_REALTY']+data['ROUND_AMT_INCOME_TOTAL']+data['ROUND_RT_CREDIT']\n",
        "#data.drop(['TMP_FLAG_OWN_REALTY'], axis=1, inplace=True)\n",
        "#data.drop(['ROUND_AMT_INCOME_TOTAL'], axis=1, inplace=True)\n",
        "#data.drop(['ROUND_RT_CREDIT'], axis=1, inplace=True)\n",
        "\n",
        "#test['TMP_FLAG_OWN_REALTY']=test['FLAG_OWN_REALTY']\n",
        "#test.loc[test['TMP_FLAG_OWN_REALTY'].isna(),'TMP_FLAG_OWN_REALTY']='O'\n",
        "#test['ROUND_AMT_INCOME_TOTAL']=round(test['AMT_INCOME_TOTAL']/100000)\n",
        "#test['ROUND_AMT_INCOME_TOTAL'].fillna(0,inplace=True)\n",
        "#test.loc[test['ROUND_AMT_INCOME_TOTAL']>6,'ROUND_AMT_INCOME_TOTAL']=6\n",
        "#test['ROUND_AMT_INCOME_TOTAL']=test['ROUND_AMT_INCOME_TOTAL'].astype(str)\n",
        "#test['ROUND_RT_CREDIT']=round(test['RT_CREDIT'])\n",
        "#test.loc[test['ROUND_RT_CREDIT']>6,'ROUND_RT_CREDIT']=6\n",
        "#test['ROUND_RT_CREDIT'].fillna(0,inplace=True)\n",
        "#test['ROUND_RT_CREDIT']=test['ROUND_RT_CREDIT'].astype(str)\n",
        "#test['REALTY_INCOME_CREDIT']=test['TMP_FLAG_OWN_REALTY']+test['ROUND_AMT_INCOME_TOTAL']+test['ROUND_RT_CREDIT']\n",
        "\n",
        "#data['ROUND_EXT_SOURCE_1']=round(data['EXT_SOURCE_1']*100)\n",
        "#data['ROUND_EXT_SOURCE_2']=round(data['EXT_SOURCE_2']*100)\n",
        "#data['ROUND_EXT_SOURCE_3']=round(data['EXT_SOURCE_3']*100)\n",
        "\n",
        "#test['ROUND_EXT_SOURCE_1']=round(test['EXT_SOURCE_1']*100)\n",
        "#test['ROUND_EXT_SOURCE_2']=round(test['EXT_SOURCE_2']*100)\n",
        "#test['ROUND_EXT_SOURCE_3']=round(test['EXT_SOURCE_3']*100)\n",
        "\n",
        "t1=data.loc[data['TARGET']==1]\n",
        "t0=data.loc[data['TARGET']==0]\n",
        "ta=data\n",
        "def addColumnOfNumber(column_target):\n",
        "    column_new = 'NEW_NUM_'+column_target\n",
        "    cats = ta[column_target].unique()\n",
        "\n",
        "    dict={}\n",
        "    for cat in cats:\n",
        "      dict[cat]=(t1.loc[ta[column_target]==cat,'SK_ID_CURR'].count()/ta.loc[ta[column_target]==cat,'SK_ID_CURR'].count()).astype(str)\n",
        "\n",
        "    data[column_new]=data[column_target]\n",
        "    test[column_new]=test[column_target]\n",
        "\n",
        "    data[column_new]=data[column_new].map(dict)\n",
        "    test[column_new]=test[column_new].map(dict)\n",
        "\n",
        "    data[column_new]=data[column_new].astype(float)\n",
        "    test[column_new]=test[column_new].astype(float)\n",
        "\n",
        "#addColumnOfNumber('REALTY_INCOME_CREDIT')\n",
        "#addColumnOfNumber('ROUND_EXT_SOURCE_1')\n",
        "#addColumnOfNumber('ROUND_EXT_SOURCE_2')\n",
        "#addColumnOfNumber('ROUND_EXT_SOURCE_3')\n",
        "\n",
        "# Create an anomalous flag column\n",
        "data['DAYS_EMPLOYED_ANOM'] = data[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "data['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "# Create an anomalous flag column\n",
        "test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "data['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
        "test['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
        "\n",
        "data['YEARS_LAST_PHONE_CHANGE']=round(data['DAYS_LAST_PHONE_CHANGE']/365)\n",
        "test['YEARS_LAST_PHONE_CHANGE']=round(test['DAYS_LAST_PHONE_CHANGE']/365)\n",
        "\n",
        "data['MONTH_LAST_PHONE_CHANGE']=round(data['DAYS_LAST_PHONE_CHANGE']/30)\n",
        "test['MONTH_LAST_PHONE_CHANGE']=round(test['DAYS_LAST_PHONE_CHANGE']/30)\n",
        "\n",
        "#Separate target variable\n",
        "y = data['TARGET']\n",
        "del data['TARGET']\n",
        "\n",
        "#One-hot encoding of categorical features in data and test sets\n",
        "categorical_features = [col for col in data.columns if data[col].dtype == 'object']\n",
        "\n",
        "one_hot_df = pd.concat([data,test])\n",
        "one_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n",
        "\n",
        "data = one_hot_df.iloc[:data.shape[0],:]\n",
        "test = one_hot_df.iloc[data.shape[0]:,]\n",
        "\n",
        "# Align data and test\n",
        "\n",
        "data_labels = y\n",
        "\n",
        "# Align the dataing and testing data, keep only columns present in both dataframes\n",
        "data, test = data.align(test, join = 'inner', axis = 1)\n",
        "\n",
        "# Add the target back in\n",
        "data['TARGET'] = y\n",
        "\n",
        "print('dataing Features shape: ', data.shape)\n",
        "print('Testing Features shape: ', test.shape)\n",
        "\n",
        "test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n",
        "test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "print('There are %d anomalies in the test data out of %d entries' % (test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(test)))\n",
        "\n",
        "#data['EXT_SOURCE_1_y']\n",
        "\n",
        "#Polynomial Features\n",
        "\n",
        "# Make a new dataframe for polynomial features\n",
        "poly_features = data[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
        "poly_features_test = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "\n",
        "# imputer for handling missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "poly_target = poly_features['TARGET']\n",
        "\n",
        "poly_features = poly_features.drop(columns = ['TARGET'])\n",
        "\n",
        "# Need to impute missing values\n",
        "poly_features = imputer.fit_transform(poly_features)\n",
        "poly_features_test = imputer.transform(poly_features_test)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "                                  \n",
        "# Create the polynomial object with specified degree\n",
        "poly_transformer = PolynomialFeatures(degree = 3)\n",
        "\n",
        "# data the polynomial features\n",
        "poly_transformer.fit(poly_features)\n",
        "\n",
        "# Transform the features\n",
        "poly_features = poly_transformer.transform(poly_features)\n",
        "poly_features_test = poly_transformer.transform(poly_features_test)\n",
        "print('Polynomial Features shape: ', poly_features.shape)\n",
        "\n",
        "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]\n",
        "\n",
        "# Create a dataframe of the features \n",
        "poly_features = pd.DataFrame(poly_features, \n",
        "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
        "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Add in the target\n",
        "poly_features['TARGET'] = poly_target\n",
        "\n",
        "# Find the correlations with the target\n",
        "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display most negative and most positive\n",
        "print(poly_corrs.head(20))\n",
        "print(poly_corrs.tail(20))\n",
        "\n",
        "# Put test features into dataframe\n",
        "poly_features_test = pd.DataFrame(poly_features_test, \n",
        "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
        "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
        "\n",
        "# Merge polynomial features into dataing dataframe\n",
        "poly_features['SK_ID_CURR'] = data['SK_ID_CURR']\n",
        "app_train_poly = data.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Merge polnomial features into testing dataframe\n",
        "poly_features_test['SK_ID_CURR'] = test['SK_ID_CURR']\n",
        "app_test_poly = test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
        "\n",
        "# Align the dataframes\n",
        "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
        "\n",
        "# Print out the new shapes\n",
        "print('dataing data with polynomial features shape: ', app_train_poly.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_poly.shape)\n",
        "\n",
        "app_train_poly.head()\n",
        "\n",
        "app_train_poly=app_train_poly.rename(columns={'EXT_SOURCE_1_x':'EXT_SOURCE_1','EXT_SOURCE_2_x':'EXT_SOURCE_2','EXT_SOURCE_3_x':'EXT_SOURCE_3','DAYS_BIRTH_x':'DAYS_BIRTH'})\n",
        "#app_train_poly=app_train_poly.drop('1',inplace=True)\n",
        "\n",
        "app_test_poly=app_test_poly.rename(columns={'EXT_SOURCE_1_x':'EXT_SOURCE_1','EXT_SOURCE_2_x':'EXT_SOURCE_2','EXT_SOURCE_3_x':'EXT_SOURCE_3','DAYS_BIRTH_x':'DAYS_BIRTH'})\n",
        "\n",
        "app_test_poly.drop('1',axis=1,inplace=True)\n",
        "app_train_poly.drop('1',axis=1,inplace=True)\n",
        "\n",
        "#app_train_poly['NAME_TYPE_SUITE_Spouse, partner_x']\n",
        "\n",
        "# check and remove constant columns\n",
        "#colsToRemove = []\n",
        "#for col in app_train_poly.columns:\n",
        "#    if col != 'SK_ID_CURR' and col != 'TARGET':\n",
        "#        if app_train_poly[col].std() == 0: \n",
        "#            colsToRemove.append(col)\n",
        "        \n",
        "# remove constant columns in the training set\n",
        "#app_train_poly.drop(colsToRemove, axis=1, inplace=True)\n",
        "\n",
        "# remove constant columns in the test set\n",
        "#app_test_poly.drop(colsToRemove, axis=1, inplace=True) \n",
        "\n",
        "#print(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\n",
        "#print(colsToRemove)\n",
        "\n",
        "#app_train_domain['SK_ID_CURR']\n",
        "\n",
        "# create temp DF\n",
        "#data1 = pd.read_csv('../input/application_train.csv')\n",
        "#test1 = pd.read_csv('../input/application_test.csv')\n",
        "\n",
        "#app_train_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)\n",
        "#app_test_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)\n",
        "\n",
        "#app_train_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)\n",
        "#app_test_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)\n",
        "\n",
        "#app_train_poly['AMT_CREDIT'] = data1['AMT_CREDIT']\n",
        "#app_test_poly['AMT_CREDIT'] = test1['AMT_CREDIT']\n",
        "#app_train_poly['AMT_GOODS_PRICE'] = data1['AMT_GOODS_PRICE']\n",
        "#app_test_poly['AMT_GOODS_PRICE'] = test1['AMT_GOODS_PRICE']\n",
        "app_train_poly=app_train_poly.rename(columns={'AMT_CREDIT_x':'AMT_CREDIT','AMT_GOODS_PRICE_x':'AMT_GOODS_PRICE'})\n",
        "app_test_poly=app_test_poly.rename(columns={'AMT_CREDIT_x':'AMT_CREDIT','AMT_GOODS_PRICE_x':'AMT_GOODS_PRICE'})\n",
        "\n",
        "#app_train_poly['AMT_ANNUITY_x']\n",
        "\n",
        "app_train_domain = app_train_poly.copy()\n",
        "app_test_domain = app_test_poly.copy()\n",
        "\n",
        "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
        "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n",
        "app_train_domain['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_ANNUITY']\n",
        "app_train_domain['NEW_CREDIT_TO_GOODS_RATIO'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_GOODS_PRICE']\n",
        "app_train_domain['NEW_EXT_SOURCES_MEAN'] = app_train_domain[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "app_train_domain['NEW_EMPLOY_TO_BIRTH_RATIO'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n",
        "#\n",
        "app_train_domain['NEW_EMPLOY_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_EMPLOYED']\n",
        "app_train_domain['NEW_PHONE_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_REGISTRATION_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_REGISTRATION']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_BIRTH_NUM'] = app_train_domain['DAYS_BIRTH'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_PHONE_TO_REGISTRATION_NUM'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_EMPLOY_TO_REGISTRATION_NUM'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_EMPLOYED']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_REGISTRATION_NUM'] = app_train_domain['DAYS_REGISTRATION'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_PHONE_TO_EMPLOY_NUM'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_EMPLOY_NUM'] = app_train_domain['DAYS_EMPLOYED'] - app_train_domain['DAYS_ID_PUBLISH']\n",
        "app_train_domain['NEW_REGION_RATING_CLIENT_NUM'] = app_train_domain['REGION_RATING_CLIENT_W_CITY'] - app_train_domain['REGION_RATING_CLIENT']\n",
        "app_train_domain['NEW_FAM_MEMBERS_NUM'] = app_train_domain['CNT_FAM_MEMBERS'] - app_train_domain['CNT_CHILDREN']\n",
        "app_train_domain['NEW_EMPLOY_TO_BIRTH_YEAR'] = app_train_domain['YEARS_BIRTH'] - app_train_domain['YEARS_EMPLOYED']\n",
        "app_train_domain['NEW_PHONE_TO_BIRTH_YEAR'] = app_train_domain['YEARS_BIRTH'] - app_train_domain['YEARS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_REGISTRATION_TO_BIRTH_YEAR'] = app_train_domain['YEARS_BIRTH'] - app_train_domain['YEARS_REGISTRATION']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_BIRTH_YEAR'] = app_train_domain['YEARS_BIRTH'] - app_train_domain['YEARS_ID_PUBLISH']\n",
        "app_train_domain['NEW_OWN_CAR_TO_BIRTH_YEAR'] = app_train_domain['YEARS_BIRTH'] - app_train_domain['OWN_CAR_AGE']\n",
        "app_train_domain['NEW_PHONE_TO_REGISTRATION_YEAR'] = app_train_domain['YEARS_REGISTRATION'] - app_train_domain['YEARS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_EMPLOY_TO_REGISTRATION_YEAR'] = app_train_domain['YEARS_REGISTRATION'] - app_train_domain['YEARS_EMPLOYED']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_REGISTRATION_YEAR'] = app_train_domain['YEARS_REGISTRATION'] - app_train_domain['YEARS_ID_PUBLISH']\n",
        "app_train_domain['NEW_OWN_CAR_TO_REGISTRATION_YEAR'] = app_train_domain['YEARS_REGISTRATION'] - app_train_domain['OWN_CAR_AGE']\n",
        "app_train_domain['NEW_PHONE_TO_EMPLOY_YEAR'] = app_train_domain['YEARS_EMPLOYED'] - app_train_domain['YEARS_LAST_PHONE_CHANGE']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_EMPLOY_YEAR'] = app_train_domain['YEARS_EMPLOYED'] - app_train_domain['YEARS_ID_PUBLISH']\n",
        "app_train_domain['NEW_OWN_CAR_TO_EMPLOY_YEAR'] = app_train_domain['YEARS_EMPLOYED'] - app_train_domain['OWN_CAR_AGE']\n",
        "app_train_domain['NEW_ID_PUBLISH_TO_OWN_CAR'] = app_train_domain['OWN_CAR_AGE'] - app_train_domain['YEARS_ID_PUBLISH']\n",
        "\n",
        "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
        "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']\n",
        "\n",
        "app_test_domain['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_ANNUITY']\n",
        "app_test_domain['NEW_CREDIT_TO_GOODS_RATIO'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_GOODS_PRICE']\n",
        "app_test_domain['NEW_EXT_SOURCES_MEAN'] = app_test_domain[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "app_test_domain['NEW_EMPLOY_TO_BIRTH_RATIO'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']\n",
        "\n",
        "#\n",
        "app_test_domain['NEW_EMPLOY_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_EMPLOYED']\n",
        "app_test_domain['NEW_PHONE_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_REGISTRATION_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_REGISTRATION']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_BIRTH_NUM'] = app_test_domain['DAYS_BIRTH'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_PHONE_TO_REGISTRATION_NUM'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_EMPLOY_TO_REGISTRATION_NUM'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_EMPLOYED']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_REGISTRATION_NUM'] = app_test_domain['DAYS_REGISTRATION'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_PHONE_TO_EMPLOY_NUM'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['DAYS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_EMPLOY_NUM'] = app_test_domain['DAYS_EMPLOYED'] - app_test_domain['DAYS_ID_PUBLISH']\n",
        "app_test_domain['NEW_REGION_RATING_CLIENT_NUM'] = app_test_domain['REGION_RATING_CLIENT_W_CITY'] - app_test_domain['REGION_RATING_CLIENT']\n",
        "app_test_domain['NEW_FAM_MEMBERS_NUM'] = app_test_domain['CNT_FAM_MEMBERS'] - app_test_domain['CNT_CHILDREN']\n",
        "app_test_domain['NEW_EMPLOY_TO_BIRTH_YEAR'] = app_test_domain['YEARS_BIRTH'] - app_test_domain['YEARS_EMPLOYED']\n",
        "app_test_domain['NEW_PHONE_TO_BIRTH_YEAR'] = app_test_domain['YEARS_BIRTH'] - app_test_domain['YEARS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_REGISTRATION_TO_BIRTH_YEAR'] = app_test_domain['YEARS_BIRTH'] - app_test_domain['YEARS_REGISTRATION']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_BIRTH_YEAR'] = app_test_domain['YEARS_BIRTH'] - app_test_domain['YEARS_ID_PUBLISH']\n",
        "app_test_domain['NEW_OWN_CAR_TO_BIRTH_YEAR'] = app_test_domain['YEARS_BIRTH'] - app_test_domain['OWN_CAR_AGE']\n",
        "app_test_domain['NEW_PHONE_TO_REGISTRATION_YEAR'] = app_test_domain['YEARS_REGISTRATION'] - app_test_domain['YEARS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_EMPLOY_TO_REGISTRATION_YEAR'] = app_test_domain['YEARS_REGISTRATION'] - app_test_domain['YEARS_EMPLOYED']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_REGISTRATION_YEAR'] = app_test_domain['YEARS_REGISTRATION'] - app_test_domain['YEARS_ID_PUBLISH']\n",
        "app_test_domain['NEW_OWN_CAR_TO_REGISTRATION_YEAR'] = app_test_domain['YEARS_REGISTRATION'] - app_test_domain['OWN_CAR_AGE']\n",
        "app_test_domain['NEW_PHONE_TO_EMPLOY_YEAR'] = app_test_domain['YEARS_EMPLOYED'] - app_test_domain['YEARS_LAST_PHONE_CHANGE']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_EMPLOY_YEAR'] = app_test_domain['YEARS_EMPLOYED'] - app_test_domain['YEARS_ID_PUBLISH']\n",
        "app_test_domain['NEW_OWN_CAR_TO_EMPLOY_YEAR'] = app_test_domain['YEARS_EMPLOYED'] - app_test_domain['OWN_CAR_AGE']\n",
        "app_test_domain['NEW_ID_PUBLISH_TO_OWN_CAR'] = app_test_domain['OWN_CAR_AGE'] - app_test_domain['YEARS_ID_PUBLISH']\n",
        "\n",
        "app_train_domain['TARGET'] = poly_target\n",
        "print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_domain.shape)\n",
        "\n",
        "#app_train_domain = app_train_domain[app_train_domain.columns[app_train_domain.isnull().mean() < 0.80]]\n",
        "#app_test_domain = app_test_domain[app_test_domain.columns[app_test_domain.isnull().mean() < 0.80]]\n",
        "#print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "#print('Testing data with polynomial features shape:  ', app_test_domain.shape)\n",
        "\n",
        "#app_train_domain = app_test_domain.drop('AMT_ANNUITY',axis=1)\n",
        "#app_test_domain = app_test_domain.drop('AMT_ANNUITY',axis=1)\n",
        "\n",
        "#app_train_domain['TARGET']\n",
        "\n",
        "app_train_domain['TARGET'] = poly_target\n",
        "print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_domain.shape)\n",
        "\n",
        "#app_train_domain = app_train_domain.reindex(\n",
        " #   np.random.permutation(app_train_domain.index))\n",
        "\n",
        "#app_train_domain=app_train_domain.drop('TARGET',axis=1)\n",
        "#app_train_domain = np.log1p(app_train_domain)\n",
        "#app_test_domain=np.log1p(app_test_domain)\n",
        "#print('Training data with polynomial features shape: ', app_train_domain.shape)\n",
        "#print('Testing data with polynomial features shape:  ', app_test_domain.shape)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "\n",
        "def model(features, test_features, encoding = 'ohe', n_folds =5 ):\n",
        "    \n",
        "    \"\"\"Train and test a light gradient boosting model using\n",
        "    cross validation. \n",
        "    \n",
        "    Parameters\n",
        "    --------\n",
        "        features (pd.DataFrame): \n",
        "            dataframe of training features to use \n",
        "            for training a model. Must include the TARGET column.\n",
        "        test_features (pd.DataFrame): \n",
        "            dataframe of testing features to use\n",
        "            for making predictions with the model. \n",
        "        encoding (str, default = 'ohe'): \n",
        "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
        "            n_folds (int, default = 5): number of folds to use for cross validation\n",
        "        \n",
        "    Return\n",
        "    --------\n",
        "        submission (pd.DataFrame): \n",
        "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
        "            predicted by the model.\n",
        "        feature_importances (pd.DataFrame): \n",
        "            dataframe with the feature importances from the model.\n",
        "        valid_metrics (pd.DataFrame): \n",
        "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract the ids\n",
        "    train_ids = features['SK_ID_CURR']\n",
        "    test_ids = test_features['SK_ID_CURR']\n",
        "    \n",
        "    # Extract the labels for training\n",
        "    labels = features['TARGET']\n",
        "    \n",
        "    # Remove the ids and target\n",
        "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
        "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
        "    \n",
        "    \n",
        "    # One Hot Encoding\n",
        "    if encoding == 'ohe':\n",
        "        features = pd.get_dummies(features)\n",
        "        test_features = pd.get_dummies(test_features)\n",
        "        \n",
        "        # Align the dataframes by the columns\n",
        "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
        "        \n",
        "        # No categorical indices to record\n",
        "        cat_indices = 'auto'\n",
        "    \n",
        "    # Integer label encoding\n",
        "    elif encoding == 'le':\n",
        "        \n",
        "        # Create a label encoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        \n",
        "        # List for storing categorical indices\n",
        "        cat_indices = []\n",
        "        \n",
        "        # Iterate through each column\n",
        "        for i, col in enumerate(features):\n",
        "            if features[col].dtype == 'object':\n",
        "                # Map the categorical features to integers\n",
        "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
        "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
        "\n",
        "                # Record the categorical indices\n",
        "                cat_indices.append(i)\n",
        "    \n",
        "    # Catch error if label encoding scheme is not valid\n",
        "    else:\n",
        "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
        "        \n",
        "    print('Training Data Shape: ', features.shape)\n",
        "    print('Testing Data Shape: ', test_features.shape)\n",
        "    \n",
        "    # Extract feature names\n",
        "    feature_names = list(features.columns)\n",
        "    \n",
        "    # Convert to np arrays\n",
        "    features = np.array(features)\n",
        "    test_features = np.array(test_features)\n",
        "    \n",
        "    # Create the kfold object\n",
        "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
        "    \n",
        "    # Empty array for feature importances\n",
        "    feature_importance_values = np.zeros(len(feature_names))\n",
        "    \n",
        "    # Empty array for test predictions\n",
        "    test_predictions = np.zeros(test_features.shape[0])\n",
        "    \n",
        "    # Empty array for out of fold validation predictions\n",
        "    out_of_fold = np.zeros(features.shape[0])\n",
        "    \n",
        "    # Lists for recording validation and training scores\n",
        "    valid_scores = []\n",
        "    train_scores = []\n",
        "    \n",
        "    # Iterate through each fold\n",
        "    for train_indices, valid_indices in k_fold.split(features):\n",
        "        \n",
        "        # Training data for the fold\n",
        "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
        "        # Validation data for the fold\n",
        "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
        "        \n",
        "        # Create the model\n",
        "        model = lgb.LGBMClassifier(n_estimators=10000, nthread=4,objective = 'binary', \n",
        "                                   class_weight = 'balanced', learning_rate = 0.015, \n",
        "                                   reg_alpha = 0.041545473, reg_lambda = 0.1, \n",
        "                                   n_jobs = -1, random_state = 50,num_leaves=32,colsample_bytree=.9497036,subsample=.8715623,\n",
        "                                  max_depth=5,min_split_gain=.0222415,min_child_weight=39.3259775,max_bin=200,num_boost_round=3000,min_data_in_leaf=100,bagging_fraction=0.5,bagging_freq=10)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
        "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
        "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
        "                  early_stopping_rounds = 100, verbose = 200)\n",
        "        \n",
        "        # Record the best iteration\n",
        "        best_iteration = model.best_iteration_\n",
        "        \n",
        "        # Record the feature importances\n",
        "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
        "        \n",
        "        # Make predictions\n",
        "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
        "        \n",
        "        # Record the out of fold predictions\n",
        "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
        "        \n",
        "        # Record the best score\n",
        "        valid_score = model.best_score_['valid']['auc']\n",
        "        train_score = model.best_score_['train']['auc']\n",
        "        \n",
        "        valid_scores.append(valid_score)\n",
        "        train_scores.append(train_score)\n",
        "        \n",
        "        # Clean up memory\n",
        "        gc.enable()\n",
        "        del model, train_features, valid_features\n",
        "        gc.collect()\n",
        "        \n",
        "    # Make the submission dataframe\n",
        "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
        "    \n",
        "    # Make the feature importance dataframe\n",
        "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
        "    \n",
        "    # Overall validation score\n",
        "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
        "    \n",
        "    # Add the overall scores to the metrics\n",
        "    valid_scores.append(valid_auc)\n",
        "    train_scores.append(np.mean(train_scores))\n",
        "    \n",
        "    # Needed for creating dataframe of validation scores\n",
        "    fold_names = list(range(n_folds))\n",
        "    fold_names.append('overall')\n",
        "    \n",
        "    # Dataframe of validation scores\n",
        "    metrics = pd.DataFrame({'fold': fold_names,\n",
        "                            'train': train_scores,\n",
        "                            'valid': valid_scores}) \n",
        "    \n",
        "    return submission, feature_importances, metrics\n",
        "\n",
        "submission, fi, metrics = model(app_train_domain, app_test_domain)\n",
        "print('Baseline metrics')\n",
        "print(metrics)\n",
        "\n",
        "\"\"\"June27_11:00\n",
        "      fold     train     valid\n",
        "\n",
        "* 0    0.812380    0.763470\n",
        "* 1    0.800873    0.761361\n",
        "* 2    0.829364    0.763497\n",
        "* 3    0.804734    0.749407\n",
        "* 4    0.808092    0.752702\n",
        "*  overall  0.811088  0.758063\n",
        "\n",
        "This is 0.76335 on leader board\n",
        "\n",
        "June27　1530　（Ext_1-3を100で導入）\n",
        "      fold     train     valid\n",
        "*        0  0.812034  0.763665\n",
        "*        1  0.814457  0.761841\n",
        "*        2  0.795298  0.761181\n",
        "*        3  0.804273  0.749477\n",
        "*        4  0.806116  0.753385\n",
        "*  overall  0.806436  0.757831\n",
        "\n",
        "June27 1630 (Realty_Income_Credit_Num)\n",
        "\n",
        "      fold     train     valid\n",
        "*        0  0.811275  0.764974\n",
        "*        1  0.810877  0.763801\n",
        "*        2  0.806269  0.763429\n",
        "*        3  0.818493  0.751784\n",
        "*        4  0.807860  0.754887\n",
        "*  overall  0.810955  0.759639\n",
        "\n",
        "June27 1700 (Ext1_3 Num)\n",
        "\n",
        "      fold     train     valid\n",
        "*        0  0.812888  0.763947\n",
        "*        1  0.803323  0.762918\n",
        "*        2  0.821568  0.765399\n",
        "*        3  0.817804  0.753279\n",
        "*        4  0.808146  0.754978\n",
        "*  overall  0.812746  0.760012\n",
        "\n",
        "This is 0.7596 on Public Leader Board\n",
        "\n",
        "      fold     train     valid\n",
        "*        0  0.811275  0.764974\n",
        "*        1  0.810877  0.763801\n",
        "*        2  0.806269  0.763429\n",
        "*        3  0.818493  0.751784\n",
        "*        4  0.807860  0.754887\n",
        "*  overall  0.810955  0.759639\n",
        "\n",
        "This is 0.75981 on Public Leader Board\n",
        "\n",
        "This is 0.76315 on Public Leader Board\n",
        "\n",
        "      fold     train     valid\n",
        "*        0  0.811888  0.763755\n",
        "*        1  0.807078  0.761591\n",
        "*        2  0.829285  0.763293\n",
        "*        3  0.804734  0.749407\n",
        "*        4  0.808156  0.752579\n",
        "*  overall  0.812228  0.758108\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def plot_feature_importances(df):\n",
        "    \"\"\"\n",
        "    Plot importances returned by a model. This can work with any measure of\n",
        "    feature importance provided that higher importance is better. \n",
        "    \n",
        "    Args:\n",
        "        df (dataframe): feature importances. Must have the features in a column\n",
        "        called `features` and the importances in a column called `importance\n",
        "        \n",
        "    Returns:\n",
        "        shows a plot of the 15 most importance features\n",
        "        \n",
        "        df (dataframe): feature importances sorted by importance (highest to lowest) \n",
        "        with a column for normalized importance\n",
        "        \"\"\"\n",
        "    \n",
        "    # Sort features according to importance\n",
        "    df = df.sort_values('importance', ascending = False).reset_index()\n",
        "    \n",
        "    # Normalize the feature importances to add up to one\n",
        "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
        "\n",
        "    # Make a horizontal bar chart of feature importances\n",
        "    plt.figure(figsize = (20, 20))\n",
        "    ax = plt.subplot()\n",
        "    \n",
        "    # Need to reverse the index to plot most important on top\n",
        "    ax.barh(list(reversed(list(df.index[:50]))), \n",
        "            df['importance_normalized'].head(50), \n",
        "            align = 'center', edgecolor = 'k')\n",
        "    \n",
        "    # Set the yticks and labels\n",
        "    ax.set_yticks(list(reversed(list(df.index[:50]))))\n",
        "    ax.set_yticklabels(df['feature'].head(50))\n",
        "    \n",
        "    # Plot labeling\n",
        "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
        "    plt.show()\n",
        "    \n",
        "    return df\n",
        "\n",
        "fi_sorted = plot_feature_importances(fi)\n",
        "\n",
        "submission.to_csv('second_sub.csv', index = False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfc19-RTY2zt",
        "outputId": "8c46112b-4663-49b3-e145-6f95b3a71743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "dataing Features shape:  (171202, 201)\n",
            "Testing Features shape:  (61500, 200)\n",
            "There are 0 anomalies in the test data out of 61500 entries\n",
            "Polynomial Features shape:  (171202, 35)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXT_SOURCE_2 EXT_SOURCE_3                -0.190619\n",
            "EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3   -0.187364\n",
            "EXT_SOURCE_2^2 EXT_SOURCE_3              -0.174604\n",
            "EXT_SOURCE_2 EXT_SOURCE_3^2              -0.169516\n",
            "EXT_SOURCE_1 EXT_SOURCE_2                -0.165062\n",
            "EXT_SOURCE_2                             -0.162383\n",
            "EXT_SOURCE_1 EXT_SOURCE_2^2              -0.155854\n",
            "EXT_SOURCE_2^2                           -0.151487\n",
            "EXT_SOURCE_1 EXT_SOURCE_3                -0.150418\n",
            "EXT_SOURCE_3                             -0.142190\n",
            "EXT_SOURCE_2^3                           -0.142178\n",
            "EXT_SOURCE_1 EXT_SOURCE_3^2              -0.138980\n",
            "EXT_SOURCE_1^2 EXT_SOURCE_2              -0.138804\n",
            "EXT_SOURCE_2 DAYS_BIRTH^2                -0.135825\n",
            "EXT_SOURCE_3^2                           -0.129820\n",
            "EXT_SOURCE_1^2 EXT_SOURCE_3              -0.127705\n",
            "EXT_SOURCE_3 DAYS_BIRTH^2                -0.120932\n",
            "EXT_SOURCE_3^3                           -0.117608\n",
            "EXT_SOURCE_1 DAYS_BIRTH^2                -0.090729\n",
            "EXT_SOURCE_1                             -0.081994\n",
            "Name: TARGET, dtype: float64\n",
            "EXT_SOURCE_3 DAYS_BIRTH^2              -0.120932\n",
            "EXT_SOURCE_3^3                         -0.117608\n",
            "EXT_SOURCE_1 DAYS_BIRTH^2              -0.090729\n",
            "EXT_SOURCE_1                           -0.081994\n",
            "DAYS_BIRTH^2                           -0.077987\n",
            "EXT_SOURCE_1^2                         -0.075605\n",
            "EXT_SOURCE_1^3                         -0.068726\n",
            "DAYS_BIRTH^3                            0.075583\n",
            "DAYS_BIRTH                              0.079541\n",
            "EXT_SOURCE_1^2 DAYS_BIRTH               0.089520\n",
            "EXT_SOURCE_1 DAYS_BIRTH                 0.097990\n",
            "EXT_SOURCE_3^2 DAYS_BIRTH               0.133807\n",
            "EXT_SOURCE_3 DAYS_BIRTH                 0.141401\n",
            "EXT_SOURCE_1 EXT_SOURCE_3 DAYS_BIRTH    0.143557\n",
            "EXT_SOURCE_2^2 DAYS_BIRTH               0.152261\n",
            "EXT_SOURCE_1 EXT_SOURCE_2 DAYS_BIRTH    0.157203\n",
            "EXT_SOURCE_2 DAYS_BIRTH                 0.159863\n",
            "EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH    0.179790\n",
            "TARGET                                  1.000000\n",
            "1                                            NaN\n",
            "Name: TARGET, dtype: float64\n",
            "dataing data with polynomial features shape:  (171202, 235)\n",
            "Testing data with polynomial features shape:   (61500, 235)\n",
            "Training data with polynomial features shape:  (171202, 267)\n",
            "Testing data with polynomial features shape:   (61500, 266)\n",
            "Training data with polynomial features shape:  (171202, 267)\n",
            "Testing data with polynomial features shape:   (61500, 266)\n",
            "Training Data Shape:  (171202, 265)\n",
            "Testing Data Shape:  (61500, 265)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[200]\ttrain's auc: 0.766971\ttrain's binary_logloss: 0.580479\tvalid's auc: 0.751337\tvalid's binary_logloss: 0.581559\n",
            "[400]\ttrain's auc: 0.783196\ttrain's binary_logloss: 0.562322\tvalid's auc: 0.759142\tvalid's binary_logloss: 0.566889\n",
            "[600]\ttrain's auc: 0.795081\ttrain's binary_logloss: 0.550665\tvalid's auc: 0.761109\tvalid's binary_logloss: 0.560158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 良い成績の結果と属性を結合"
      ],
      "metadata": {
        "id": "vFarcuMYcN0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lower=pd.read_csv('lower_sub.csv')\n",
        "#lower.rename(columns={'SK_ID_CURR':'LOW_ID','TARGET':'LOW_RES'},inplace=True)\n",
        "submission.rename(columns={'SK_ID_CURR':'HIGH_ID','TARGET':'HIGH_RES'},inplace=True)\n",
        "#df = pd.concat([lower,submission,test],axis=1)\n",
        "df = pd.concat([submission,test],axis=1)"
      ],
      "metadata": {
        "id": "zoy8w-lDbQuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['HIGH_ID'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "jTrB0d5Kbs-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "リボルビング・ローンの10回返済のデフォルト率はゼロである。"
      ],
      "metadata": {
        "id": "nsWbONsk0Eec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['TARGET']=df['HIGH_RES']\n",
        "df.loc[(df['NAME_CONTRACT_TYPE']!='Cash loans')&(df['AMT_CREDIT'])/df['AMT_ANNUITY']<15),'TARGET']=0"
      ],
      "metadata": {
        "id": "GIC9YK0hgeOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final=df[['SK_ID_CURR','TARGET']]\n",
        "final.to_csv('final.csv',index=False)"
      ],
      "metadata": {
        "id": "mfK60UR9k9Gk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}