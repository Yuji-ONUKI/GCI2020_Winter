{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "July021900_micro-model-174-features-0-8-auc-on-home-credit.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuji-ONUKI/GCI2020_Winter/blob/main/July021900_micro_model_174_features_0_8_auc_on_home_credit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a Micro Model Study on Home Credit\n",
        "\n",
        "The Home Credit Default Risk dataset on the Kaggle is subjected as a final project of my DS/ML bootcamp, and I have spent a period of three weeks on this project. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one +0.804). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer/float dtype conversion on my datasets. In addition, I made a bleend boosting study to acheive highest AUC score (0.81128, much highers possible) on Kaggle (https://www.kaggle.com/hikmetsezen/blend-boosting-for-home-credit-default-risk).\n",
        "\n",
        "Here I would like to share my micro model study with you. This micro model has only 174 features and is able to reach better than 0.8 AUC score. Micro model is developed on my base model via successive feature elimination and addition procedure, which is developed by myself. My ambition is that tremendously increasing number of feature is not always necessary to improve performance of model! \n",
        "\n",
        "Mostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. \n",
        "\n",
        "I have a limited knowledge about the credit finance, therefore, I combined many Kaggle notebooks for expending number of features as much as I desire and/or acceptance of my LigthGBM models harvesting further enhance scores. I would like to thank these contributors. Some of them are listed here:\n",
        "* https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features <=-- my models are based on this study\n",
        "* https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution\n",
        "* https://www.kaggle.com/sangseoseo/oof-all-home-credit-default-risk <=-- in most cases these hyperparameters are used\n",
        "* https://www.kaggle.com/ashishpatel26/different-basic-blends-possible <=-- thank for blending idea\n",
        "* https://www.kaggle.com/mathchi/home-credit-risk-with-detailed-feature-engineering\n",
        "* https://www.kaggle.com/windofdl/kernelf68f763785\n",
        "* https://www.kaggle.com/meraxes10/lgbm-credit-default-prediction\n",
        "* https://www.kaggle.com/luudactam/hc-v500\n",
        "* https://www.kaggle.com/aantonova/aggregating-all-tables-in-one-dataset\n",
        "* https://www.kaggle.com/wanakon/kernel24647bb75c"
      ],
      "metadata": {
        "id": "lw9PwzlO0Rpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lightgbm==2.3.1\n",
        "# import lightgbm\n",
        "# lightgbm.__version__"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "5RKGN2JA0Rpz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import gc\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "trusted": true,
        "id": "CnuASwW10Rp1"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run functions and pre_settings\n",
        "def one_hot_encoder(df, nan_as_category=True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, new_columns\n",
        "\n",
        "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
        "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
        "                               for e in agg_df.columns.tolist()])\n",
        "    return agg_df.reset_index()\n",
        "\n",
        "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
        "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
        "\n",
        "def do_sum(dataframe, group_cols, counted, agg_name):\n",
        "    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n",
        "    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n",
        "    return dataframe\n",
        "\n",
        "def reduce_mem_usage(dataframe):\n",
        "    m_start = dataframe.memory_usage().sum() / 1024 ** 2\n",
        "    for col in dataframe.columns:\n",
        "        col_type = dataframe[col].dtype\n",
        "        if col_type != object:\n",
        "            c_min = dataframe[col].min()\n",
        "            c_max = dataframe[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    dataframe[col] = dataframe[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    dataframe[col] = dataframe[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    dataframe[col] = dataframe[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    dataframe[col] = dataframe[col].astype(np.int64)\n",
        "            elif str(col_type)[:5] == 'float':\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    dataframe[col] = dataframe[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    dataframe[col] = dataframe[col].astype(np.float32)\n",
        "                else:\n",
        "                    dataframe[col] = dataframe[col].astype(np.float64)\n",
        "\n",
        "    m_end = dataframe.memory_usage().sum() / 1024 ** 2\n",
        "    return dataframe\n",
        "\n",
        "nan_as_category = True"
      ],
      "metadata": {
        "trusted": true,
        "id": "F25iusIZ0Rp3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def application():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/GCI/02.（公開）コンペ2-20220621T094535Z-001.zip (Unzipped Files)/02.（公開）コンペ2/input/train.csv\")\n",
        "    test_df = pd.read_csv(\"/content/drive/MyDrive/GCI/02.（公開）コンペ2-20220621T094535Z-001.zip (Unzipped Files)/02.（公開）コンペ2/input/test.csv\")\n",
        "\n",
        "    df = df.append(test_df).reset_index()\n",
        "\n",
        "    # general cleaning procedures\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']\n",
        "    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M\n",
        "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # set null value\n",
        "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value\n",
        "    #\n",
        "    df['REGION_POPULATION_RELATIVE_0.04622']=0\n",
        "    df['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE_0.04622']=1\n",
        "    df['REGION_POPULATION_RELATIVE'==0.4622,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "    df['REGION_POPULATION_RELATIVE_0.072508']=0\n",
        "    df['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE_0.072508']=1\n",
        "    df['REGION_POPULATION_RELATIVE'==0.072508,'REGION_POPULATION_RELATIVE']=np.nan\n",
        "\n",
        "    df['OWN_CAR_AGE_64']=0\n",
        "    df['OWN_CAR_AGE'==64,'OWN_CAR_AGE_64']=1\n",
        "    df['OWN_CAR_AGE'==64,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "    df['OWN_CAR_AGE_65']=0\n",
        "    df['OWN_CAR_AGE'==65,'OWN_CAR_AGE_65']=1\n",
        "    df['OWN_CAR_AGE'==65,'OWN_CAR_AGE']=np.nan\n",
        "\n",
        "    # Categorical features with Binary encode (0 or 1; two categories)\n",
        "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
        "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
        "    \n",
        "    # Categorical features with One-Hot encode\n",
        "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
        "\n",
        "    # Flag_document features - count and kurtosis\n",
        "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
        "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
        "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
        "\n",
        "    def get_age_label(days_birth):\n",
        "        \"\"\" Return the age group label (int). \"\"\"\n",
        "        age_years = -days_birth / 365\n",
        "        if age_years < 27: return 1\n",
        "        elif age_years < 40: return 2\n",
        "        elif age_years < 50: return 3\n",
        "        elif age_years < 65: return 4\n",
        "        elif age_years < 99: return 5\n",
        "        else: return 0\n",
        "    # Categorical age - based on target=1 plot\n",
        "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
        "\n",
        "    # New features based on External sources\n",
        "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
        "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
        "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
        "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
        "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
        "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
        "\n",
        "    # Some simple new features (percentages)\n",
        "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
        "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "\n",
        "    # Credit ratios\n",
        "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
        "    \n",
        "    # Income ratios\n",
        "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
        "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
        "    \n",
        "    # Time ratios\n",
        "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
        "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
        "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
        "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
        "\n",
        "    # EXT_SOURCE_X FEATURE\n",
        "    df['APPS_EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "    df['APPS_EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
        "    df['APPS_EXT_SOURCE_STD'] = df['APPS_EXT_SOURCE_STD'].fillna(df['APPS_EXT_SOURCE_STD'].mean())\n",
        "    df['APP_SCORE1_TO_BIRTH_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_BIRTH'] / 365.25)\n",
        "    df['APP_SCORE2_TO_BIRTH_RATIO'] = df['EXT_SOURCE_2'] / (df['DAYS_BIRTH'] / 365.25)\n",
        "    df['APP_SCORE3_TO_BIRTH_RATIO'] = df['EXT_SOURCE_3'] / (df['DAYS_BIRTH'] / 365.25)\n",
        "    df['APP_SCORE1_TO_EMPLOY_RATIO'] = df['EXT_SOURCE_1'] / (df['DAYS_EMPLOYED'] / 365.25)\n",
        "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['DAYS_BIRTH']\n",
        "    df['APP_SCORE1_TO_FAM_CNT_RATIO'] = df['EXT_SOURCE_1'] / df['CNT_FAM_MEMBERS']\n",
        "    df['APP_SCORE1_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_GOODS_PRICE']\n",
        "    df['APP_SCORE1_TO_CREDIT_RATIO'] = df['EXT_SOURCE_1'] / df['AMT_CREDIT']\n",
        "    df['APP_SCORE1_TO_SCORE2_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_2']\n",
        "    df['APP_SCORE1_TO_SCORE3_RATIO'] = df['EXT_SOURCE_1'] / df['EXT_SOURCE_3']\n",
        "    df['APP_SCORE2_TO_CREDIT_RATIO'] = df['EXT_SOURCE_2'] / df['AMT_CREDIT']\n",
        "    df['APP_SCORE2_TO_REGION_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT']\n",
        "    df['APP_SCORE2_TO_CITY_RATING_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_RATING_CLIENT_W_CITY']\n",
        "    df['APP_SCORE2_TO_POP_RATIO'] = df['EXT_SOURCE_2'] / df['REGION_POPULATION_RELATIVE']\n",
        "    df['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df['EXT_SOURCE_2'] / df['DAYS_LAST_PHONE_CHANGE']\n",
        "    df['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n",
        "    df['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n",
        "    df['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "    df['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n",
        "    df['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n",
        "    df['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n",
        "\n",
        "    # AMT_INCOME_TOTAL : income\n",
        "    # CNT_FAM_MEMBERS  : the number of family members\n",
        "    df['APPS_GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] / df['AMT_INCOME_TOTAL']\n",
        "    df['APPS_CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    \n",
        "    # DAYS_BIRTH : Client's age in days at the time of application\n",
        "    # DAYS_EMPLOYED : How many days before the application the person started current employment\n",
        "    df['APPS_INCOME_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
        "\n",
        "    # other feature from better than 0.8\n",
        "    df['CREDIT_TO_GOODS_RATIO_2'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
        "    df['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n",
        "    df['APP_INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
        "    df['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
        "    df['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n",
        "\n",
        "    print('\"Application_Train_Test\" final shape:', df.shape)\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "id": "0KlSb_-N0Rp5"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = application()\n",
        "df = reduce_mem_usage(df)\n",
        "print('data types are converted for a reduced memory usage')\n",
        "df = df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n",
        "print('names of feature are renamed')"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMSeqXLu0RqG",
        "outputId": "bfa96924-0374-4e72-fdbe-5eec40a6de60"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\"Application_Train_Test\" final shape: (232697, 215)\n",
            "data types are converted for a reduced memory usage\n",
            "names of feature are renamed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('===============================================', '\\n', '##### the ML in processing...')\n",
        "\n",
        "    # loading predicted result \n",
        "df_sub = df.loc[df['TARGET'].isnull(),['SK_ID_CURR', 'TARGET']]\n",
        "\n",
        "\n",
        "    # split train, and test datasets\n",
        "train_df = df[df['TARGET'].notnull()]\n",
        "test_df = df[df['TARGET'].isnull()]\n",
        "del df\n",
        "\n",
        "    # Expand train dataset with two times of test dataset including predicted results\n",
        "test_df.TARGET = np.where(df_sub.TARGET > 0.75, 1, 0)\n",
        "train_df = pd.concat([train_df, test_df], axis=0)\n",
        "train_df = pd.concat([train_df, test_df], axis=0)\n",
        "print(f'Train shape: {train_df.shape}, test shape: {test_df.shape} are loaded.')\n",
        "\n",
        "\n",
        "    # Cross validation model\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "    # Create arrays and dataframes to store results\n",
        "oof_preds = np.zeros(train_df.shape[0])\n",
        "sub_preds = np.zeros(test_df.shape[0])\n",
        "\n",
        "\n",
        "    # limit number of feature to only 174!!!\n",
        "feats = ['index', 'ORGANIZATION_TYPE_Industry_type_5', 'NAME_EDUCATION_TYPE_Higher_education','REGION_RATING_CLIENT_W_CITY', 'NAME_HOUSING_TYPE_House_apartment', 'ANNUITY_INCOME_PERC', 'ORGANIZATION_TYPE_Services', 'ORGANIZATION_TYPE_Cleaning', 'ORGANIZATION_TYPE_Military',  'ORGANIZATION_TYPE_School',    'DAYS_BIRTH',  'OCCUPATION_TYPE_High_skill_tech_staff',  'OCCUPATION_TYPE_Private_service_staff',  'OCCUPATION_TYPE_HR_staff',  'CODE_GENDER','ORGANIZATION_TYPE_Advertising', 'EXT_SOURCE_3', 'OCCUPATION_TYPE_Managers', 'FLAG_OWN_REALTY',  'AMT_CREDIT', 'INCOME_PER_PERSON', 'ORGANIZATION_TYPE_Police', 'FLAG_WORK_PHONE', 'ORGANIZATION_TYPE_University', 'ORGANIZATION_TYPE_Medicine', 'ORGANIZATION_TYPE_Telecom', 'ORGANIZATION_TYPE_Housing', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL',  'REGION_POPULATION_RELATIVE', 'ORGANIZATION_TYPE_Electricity', 'REGION_RATING_CLIENT',  'DAYS_ID_PUBLISH', 'EXT_SOURCE_1', 'ORGANIZATION_TYPE_Realtor', 'OCCUPATION_TYPE_Laborers', 'ORGANIZATION_TYPE_Security', 'AMT_INCOME_TOTAL',  'PAYMENT_RATE', 'FLAG_OWN_CAR',  'ORGANIZATION_TYPE_Mobile', 'DAYS_EMPLOYED_PERC', 'INCOME_CREDIT_PERC',  'ORGANIZATION_TYPE_Postal', 'ORGANIZATION_TYPE_Insurance', 'OCCUPATION_TYPE_Accountants',  'ORGANIZATION_TYPE_Agriculture', 'EXT_SOURCE_2',  'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'ORGANIZATION_TYPE_Construction','REGION_POPULATION_RELATIVE_0.04622','REGION_POPULATION_RELATIVE_0.072508','OWN_CAR_AGE_64','OWN_CAR_AGE_65']\n",
        "\n",
        "    # print final shape of dataset to evaluate by LightGBM\n",
        "print(f'only {len(feats)} features from a total {train_df.shape[1]} features are used for ML analysis')\n",
        "\n",
        "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
        "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
        "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
        "        clf = LGBMClassifier(nthread=-1,\n",
        "                            n_estimators=5000,\n",
        "                            learning_rate=0.01,\n",
        "                            max_depth=11,\n",
        "                            num_leaves=58,\n",
        "                            colsample_bytree=0.613,\n",
        "                            subsample=0.708,\n",
        "                            max_bin=407,\n",
        "                            reg_alpha=3.564,\n",
        "                            reg_lambda=4.930,\n",
        "                            min_child_weight=6,\n",
        "                            min_child_samples=165,\n",
        "                            silent=-1,\n",
        "                            verbose=-1,)\n",
        "\n",
        "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=500, early_stopping_rounds=500)\n",
        "\n",
        "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
        "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
        "\n",
        "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
        "        del clf, train_x, train_y, valid_x, valid_y\n",
        "\n",
        "print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
        "\n",
        "    # create submission file\n",
        "test_df['TARGET'] = sub_preds\n",
        "test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)\n",
        "print('a submission file is created')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShtFBB713YtD",
        "outputId": "f8af9c51-f0dc-4191-c2b6-139227bc07b4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================== \n",
            " ##### the ML in processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (294196, 215), test shape: (61499, 215) are loaded.\n",
            "only 51 features from a total 215 features are used for ML analysis\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's auc: 0.812665\ttraining's binary_logloss: 0.157258\tvalid_1's auc: 0.768862\tvalid_1's binary_logloss: 0.164571\n",
            "[1000]\ttraining's auc: 0.839508\ttraining's binary_logloss: 0.150563\tvalid_1's auc: 0.772146\tvalid_1's binary_logloss: 0.163966\n",
            "[1500]\ttraining's auc: 0.85918\ttraining's binary_logloss: 0.145512\tvalid_1's auc: 0.772376\tvalid_1's binary_logloss: 0.164009\n",
            "Early stopping, best iteration is:\n",
            "[1127]\ttraining's auc: 0.844955\ttraining's binary_logloss: 0.149204\tvalid_1's auc: 0.772235\tvalid_1's binary_logloss: 0.163951\n",
            "Fold  1 AUC : 0.772235\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's auc: 0.81082\ttraining's binary_logloss: 0.157292\tvalid_1's auc: 0.782701\tvalid_1's binary_logloss: 0.164659\n",
            "[1000]\ttraining's auc: 0.838183\ttraining's binary_logloss: 0.150713\tvalid_1's auc: 0.785005\tvalid_1's binary_logloss: 0.163835\n",
            "[1500]\ttraining's auc: 0.858767\ttraining's binary_logloss: 0.14568\tvalid_1's auc: 0.785026\tvalid_1's binary_logloss: 0.163783\n",
            "Early stopping, best iteration is:\n",
            "[1288]\ttraining's auc: 0.850434\ttraining's binary_logloss: 0.147749\tvalid_1's auc: 0.785166\tvalid_1's binary_logloss: 0.163772\n",
            "Fold  2 AUC : 0.785166\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[1000]\ttraining's auc: 0.838625\ttraining's binary_logloss: 0.150864\tvalid_1's auc: 0.775148\tvalid_1's binary_logloss: 0.162975\n",
            "[1500]\ttraining's auc: 0.857942\ttraining's binary_logloss: 0.14601\tvalid_1's auc: 0.776468\tvalid_1's binary_logloss: 0.162751\n",
            "[2000]\ttraining's auc: 0.873936\ttraining's binary_logloss: 0.141772\tvalid_1's auc: 0.776759\tvalid_1's binary_logloss: 0.162705\n",
            "[2500]\ttraining's auc: 0.888837\ttraining's binary_logloss: 0.13773\tvalid_1's auc: 0.776459\tvalid_1's binary_logloss: 0.162807\n",
            "Early stopping, best iteration is:\n",
            "[2027]\ttraining's auc: 0.874795\ttraining's binary_logloss: 0.141543\tvalid_1's auc: 0.776794\tvalid_1's binary_logloss: 0.162695\n",
            "Fold  3 AUC : 0.776794\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's auc: 0.811394\ttraining's binary_logloss: 0.156371\tvalid_1's auc: 0.775214\tvalid_1's binary_logloss: 0.168487\n",
            "[1000]\ttraining's auc: 0.838967\ttraining's binary_logloss: 0.149776\tvalid_1's auc: 0.779861\tvalid_1's binary_logloss: 0.167462\n",
            "[1500]\ttraining's auc: 0.85929\ttraining's binary_logloss: 0.144797\tvalid_1's auc: 0.780168\tvalid_1's binary_logloss: 0.167401\n",
            "Early stopping, best iteration is:\n",
            "[1426]\ttraining's auc: 0.856534\ttraining's binary_logloss: 0.145482\tvalid_1's auc: 0.780253\tvalid_1's binary_logloss: 0.167383\n",
            "Fold  4 AUC : 0.780253\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's auc: 0.810972\ttraining's binary_logloss: 0.158048\tvalid_1's auc: 0.778567\tvalid_1's binary_logloss: 0.161505\n",
            "[1000]\ttraining's auc: 0.838298\ttraining's binary_logloss: 0.151426\tvalid_1's auc: 0.782558\tvalid_1's binary_logloss: 0.160536\n",
            "[1500]\ttraining's auc: 0.857501\ttraining's binary_logloss: 0.146682\tvalid_1's auc: 0.782778\tvalid_1's binary_logloss: 0.160445\n",
            "Early stopping, best iteration is:\n",
            "[1347]\ttraining's auc: 0.852067\ttraining's binary_logloss: 0.148034\tvalid_1's auc: 0.78288\tvalid_1's binary_logloss: 0.160444\n",
            "Fold  5 AUC : 0.782880\n",
            "Full AUC score 0.779408\n",
            "a submission file is created\n"
          ]
        }
      ]
    }
  ]
}